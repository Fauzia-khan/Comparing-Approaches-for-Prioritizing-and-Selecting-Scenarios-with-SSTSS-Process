@inproceedings{10.1109/ICSE-Companion58688.2023.00086,
author = {Lu, Chengjie},
title = {Test Scenario Generation for Autonomous Driving Systems with Reinforcement Learning},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00086},
doi = {10.1109/ICSE-Companion58688.2023.00086},
abstract = {We have seen rapid development of autonomous driving systems (ADSs) in recent years. These systems place high requirements on safety and reliability for their mass adoption, and ADS testing is one of the crucial approaches to ensure the success of ADSs. To this end, this paper presents RLTester, a novel ADS testing approach, which adopts reinforcement learning (RL) to learn critical environment configurations (i.e., test scenarios) of the operating environment of ADSs that could reveal their unsafe behaviors. To generate diverse and critical test scenarios, we defined 142 environment configuration actions, and adopted the Time-To-Collision metric to construct the reward function. Our evaluation shows that RLTester discovered a total of 256 collisions, of which 192 are unique collisions, and took on average 11.59 seconds for each collision. Further, RLTester is effective in generating more diverse test scenarios compared to a state-of-the art approach, DeepCollision.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {317–319},
numpages = {3},
keywords = {autonomous driving system testing, critical scenario, reinforcement learning},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3540250.3549152,
author = {Deng, Yao and Zheng, Xi and Zhang, Mengshi and Lou, Guannan and Zhang, Tianyi},
title = {Scenario-based test reduction and prioritization for multi-module autonomous driving systems},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3549152},
doi = {10.1145/3540250.3549152},
abstract = {When developing autonomous driving systems (ADS), developers often need to replay previously collected driving recordings to check the correctness of newly introduced changes to the system. However, simply replaying the entire recording is not necessary given the high redundancy of driving scenes in a recording (e.g., keeping the same lane for 10 minutes on a highway). In this pa- per, we propose a novel test reduction and prioritization approach for multi-module ADS. First, our approach automatically encodes frames in a driving recording to feature vectors based on a driving scene schema. Then, the given recording is sliced into segments based on the similarity of consecutive vectors. Lengthy segments are truncated to reduce the length of a recording and redundant segments with the same vector are removed. The remaining seg- ments are prioritized based on both the coverage and the rarity of driving scenes. We implemented this approach on an industry- level, multi-module ADS called Apollo and evaluated it on three road maps in various regression settings. The results show that our approach significantly reduced the original recordings by over 34% while keeping comparable test effectiveness, identifying almost all injected faults. Furthermore, our test prioritization method achieves about 22% to 39% and 41% to 53% improvements over three baselines in terms of both the average percentage of faults detected (APFD) and TOP-K.},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {82–93},
numpages = {12},
keywords = {Autonomous Driving, Regression Testing, Test Prioritization, Testing Reduction},
location = {<conf-loc>, <city>Singapore</city>, <country>Singapore</country>, </conf-loc>},
series = {ESEC/FSE 2022}
}

@article{10.1145/3635709,
author = {Giamattei, Luca and Guerriero, Antonio and Pietrantuono, Roberto and Russo, Stefano},
title = {Causality-driven Testing of Autonomous Driving Systems},
year = {2024},
issue_date = {March 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3635709},
doi = {10.1145/3635709},
abstract = {Testing Autonomous Driving Systems (ADS) is essential for safe development of self-driving cars. For thorough and realistic testing, ADS are usually embedded in a simulator and tested in interaction with the simulated environment. However, their high complexity and the multiple safety requirements lead to costly and ineffective testing. Recent techniques exploit many-objective strategies and ML to efficiently search the huge input space. Despite the indubitable advances, the need for smartening the search keep being pressing. This article presents CART (CAusal-Reasoning-driven Testing), a new technique that formulates testing as a causal reasoning task. Learning causation, unlike correlation, allows assessing the effect of actively changing an input on the output, net of possible confounding variables. CART first infers the causal relations between test inputs and outputs, then looks for promising tests by querying the learnt model. Only tests suggested by the model are run on the simulator. An extensive empirical evaluation, using Pylot as ADS and CARLA as simulator, compares CART with state-of-the-art algorithms used recently on ADS. CART shows a significant gain in exposing more safety violations and does so more efficiently. More broadly, the work opens to a wider exploitation of causal learning beside (or on top of) ML for testing-related tasks.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {mar},
articleno = {74},
numpages = {35},
keywords = {Self-driving cars, autonomous vehicles, AI testing, search-based software testing, causal reasoning}
}

@inproceedings{10.1145/3597926.3598072,
author = {Cheng, Mingfei and Zhou, Yuan and Xie, Xiaofei},
title = {BehAVExplor: Behavior Diversity Guided Testing for Autonomous Driving Systems},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3598072},
doi = {10.1145/3597926.3598072},
abstract = {Testing Autonomous Driving Systems (ADSs) is a critical task for ensuring the reliability and safety of autonomous vehicles. Existing methods mainly focus on searching for safety violations while the diversity of the generated test cases is ignored, which may generate many redundant test cases and failures. Such redundant failures can reduce testing performance and increase failure analysis costs. In this paper, we present a novel behavior-guided fuzzing technique (BehAVExplor) to explore the different behaviors of the ego vehi- cle (i.e., the vehicle controlled by the ADS under test) and detect diverse violations. Specifically, we design an efficient unsupervised model, called BehaviorMiner, to characterize the behavior of the ego vehicle. BehaviorMiner extracts the temporal features from the given scenarios and performs a clustering-based abstraction to group behaviors with similar features into abstract states. A new test case will be added to the seed corpus if it triggers new behav- iors (e.g., cover new abstract states). Due to the potential conflict between the behavior diversity and the general violation feedback, we further propose an energy mechanism to guide the seed selec- tion and the mutation. The energy of a seed quantifies how good it is. We evaluated BehAVExplor on Apollo, an industrial-level ADS, and LGSVL simulation environment. Empirical evaluation results show that BehAVExplor can effectively find more diverse violations than the state-of-the-art.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {488–500},
numpages = {13},
keywords = {Apollo, Autonomous driving systems, behavior diversity, critical scenarios, fuzzing},
location = {<conf-loc>, <city>Seattle</city>, <state>WA</state>, <country>USA</country>, </conf-loc>},
series = {ISSTA 2023}
}

@article{10.1145/3550270,
author = {Laurent, Thomas and Klikovits, Stefan and Arcaini, Paolo and Ishikawa, Fuyuki and Ventresque, Anthony},
title = {Parameter Coverage for Testing of Autonomous Driving Systems under Uncertainty},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3550270},
doi = {10.1145/3550270},
abstract = {Autonomous Driving Systems (ADSs) are promising, but must show they are secure and trustworthy before adoption. Simulation-based testing is a widely adopted approach, where the ADS is run in a simulated environment over specific scenarios. Coverage criteria specify what needs to be covered to consider the ADS sufficiently tested. However, existing criteria do not guarantee to exercise the different decisions that the ADS can make, which is essential to assess its correctness. ADSs usually compute their decisions using parameterised rule-based systems and cost functions, such as cost components or decision thresholds. In this article, we argue that the parameters characterise the decision process, as their values affect the ADS’s final decisions. Therefore, we propose parameter coverage, a criterion requiring to cover the ADS’s parameters. A scenario covers a parameter if changing its value leads to different simulation results, meaning it is relevant for the driving decisions made in the scenario. Since ADS simulators are slightly uncertain, we employ statistical methods to assess multiple simulation runs for execution difference and coverage. Experiments using the Autonomoose ADS show that the criterion discriminates between different scenarios and that the cost of computing coverage can be managed with suitable heuristics.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {apr},
articleno = {58},
numpages = {31},
keywords = {Software testing, autonomous driving, coverage criteria, mutation analysis}
}

@inproceedings{10.1109/ICSE-Companion52605.2021.00052,
author = {Shao, Jinyang},
title = {Testing object detection for autonomous driving systems via 3D reconstruction},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion52605.2021.00052},
doi = {10.1109/ICSE-Companion52605.2021.00052},
abstract = {Object detection is to identify objects from images. In autonomous driving systems, object detection serves as an intermediate module, which is used as the input of autonomous decisions for vehicles. That is, the accuracy of autonomous decisions relies on the object detection. The state-of-the-art object detection modules are designed based on Deep Neural Networks (DNNs). It is difficult to employ white-box testing on DNNs since the output of a single neuron is inexplicable. Existing work conducted metamorphic testing for object detection via image synthesis: the detected object in the original image should be detected in the new synthetic image. However, a synthetic image may not look real from humans' perspective. Even the object detection module fails in detecting such synthetic image, the failure may not reflect the ability of object detection. In this paper, we propose an automatic approach to testing object detection via 3D reconstruction of vehicles in real photos. The 3D reconstruction is developed via vanishing point estimation in photos and heuristic based image insertion. Our approach adds new objects to blank spaces in photos to synthesize images. For example, a new vehicle can be added to a photo of a road and vehicles. In this approach, the output synthetic images are expected to be more natural-looking than randomly synthesizing images. The experiment is conducting on 500 driving photos from the Apollo autonomous driving dataset.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Companion Proceedings},
pages = {117–119},
numpages = {3},
keywords = {image processing, metamorphic testing, object detection system, vanishing point},
location = {Virtual Event, Spain},
series = {ICSE '21}
}

@inproceedings{10.1145/3551349.3560430,
author = {Tian, Haoxiang and Wu, Guoquan and Yan, Jiren and Jiang, Yan and Wei, Jun and Chen, Wei and Li, Shuo and Ye, Dan},
title = {Generating Critical Test Scenarios for Autonomous Driving Systems via Influential Behavior Patterns},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3560430},
doi = {10.1145/3551349.3560430},
abstract = {Autonomous Driving Systems (ADSs) are safety-critical, and must be fully tested before being deployed on real-world roads. To comprehensively evaluate the performance of ADSs, it is essential to generate various safety-critical scenarios. Most of existing studies assess ADSs either by searching high-dimensional input space, or using simple and pre-defined test scenarios, which are not efficient or not adequate. To better test ADSs, this paper proposes to automatically generate safety-critical test scenarios for ADSs by influential behavior patterns, which are mined from real traffic trajectories. Based on influential behavior patterns, a novel scenario generation technique, CRISCO, is presented to generate safety-critical scenarios for ADSs testing. CRISCO assigns participants to perform influential behaviors to challenge the ADS. It generates different test scenarios by solving trajectory constraints, and improves the challenge of those non-critical scenarios by adding participants’ behavior from influential behavior patterns incrementally. We demonstrate CRISCO on an industrial-grade ADS platform, Baidu Apollo. The experiment results show that our approach can effectively and efficiently generate critical scenarios to crash ADS, and it exposes 13 distinct types of safety violations in 12 hours. It also outperforms two state-of-art ADS testing techniques by exposing more 5 distinct types of safety violations on the same roads.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {46},
numpages = {12},
keywords = {Autonomous vehicles, Critical scenario, Influential behavior pattern},
location = {<conf-loc>, <city>Rochester</city>, <state>MI</state>, <country>USA</country>, </conf-loc>},
series = {ASE '22}
}

@inproceedings{10.1109/ASE51524.2021.9678735,
author = {Tang, Yun and Zhou, Yuan and Zhang, Tianwei and Wu, Fenghua and Liu, Yang and Wang, Gang},
title = {Systematic testing of autonomous driving systems using map topology-based scenario classification},
year = {2022},
isbn = {9781665403375},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE51524.2021.9678735},
doi = {10.1109/ASE51524.2021.9678735},
abstract = {Autonomous Driving Systems (ADSs), which replace humans to drive vehicles, are complex software systems deployed in autonomous vehicles (AVs). Since the execution of ADSs highly relies on maps, it is essential to perform global map-based testing for ADSs to guarantee their correctness and AVs' safety in different situations. Existing methods focus more on specific scenarios rather than global testing throughout the map. Testing on a global map is challenging since the complex lane connections in a map can generate enormous scenarios. In this work, we propose Atlas, an approach to ADSs' collision avoidance testing using map topology-based scenario classification. The core insight of Atlas is to generate diverse testing scenarios by classifying junction lanes according to their topology-based interaction patterns. First, Atlas divides the junction lanes into different classes such that an ADS can execute similar collision avoidance maneuvers on the lanes in the same class. Second, for each class, Atlas selects one junction lane to construct the testing scenario and generate test cases using a genetic algorithm. Finally, we implement and evaluate Atlas on Baidu Apollo with the LGSVL simulator on the San Francisco map. Results show that Atlas exposes nine types of real issues in Apollo 6.0 and reduces the number of junction lanes for testing by 98%.},
booktitle = {Proceedings of the 36th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1342–1346},
numpages = {5},
keywords = {autonomous driving systems, collision avoidance testing, scenario classification},
location = {Melbourne, Australia},
series = {ASE '21}
}

@inproceedings{10.1145/3551349.3563239,
author = {Song, Qunying and Runeson, Per and Persson, Stefan},
title = {A Scenario Distribution Model for Effective and Efficient Testing of Autonomous Driving Systems},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3563239},
doi = {10.1145/3551349.3563239},
abstract = {While autonomous driving systems are expected to change future means of mobility and reduce road accidents, understanding intensive and complex traffic situations is essential to enable testing of such systems under realistic traffic conditions. Particularly, we need to cover more relevant driving scenarios in the test. However, we do not want to spend time and resources testing useless scenarios that never happen in the real road traffic. In this work, we propose a new model that defines the distribution of scenarios using TTC (Time-to-Collision) for the vehicle–pedestrian interactions at unsignalized crossings based on the traffic density. The scenario distribution can be used as an input for test scenario generation and selection. We validate the model using real traffic data collected in Sweden and the result indicates that the model is effective and consistently upholds the real distribution, especially for critical scenarios with TTC less than 3 seconds. We also demonstrate the use of the model by connecting it to the testing of an auto-braking function from the industry. As a first step, our contribution is a model that predicts the worst-case distribution of scenarios using TTC and provides a mandatory input for testing autonomous driving systems.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {215},
numpages = {8},
keywords = {autonomous driving systems, scenario distribution, software testing, test selection, traffic modeling},
location = {<conf-loc>, <city>Rochester</city>, <state>MI</state>, <country>USA</country>, </conf-loc>},
series = {ASE '22}
}

@inproceedings{10.1145/3533767.3534397,
author = {Guo, An and Feng, Yang and Chen, Zhenyu},
title = {LiRTest: augmenting LiDAR point clouds for automated testing of autonomous driving systems},
year = {2022},
isbn = {9781450393799},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3533767.3534397},
doi = {10.1145/3533767.3534397},
abstract = {With the tremendous advancement of Deep Neural Networks (DNNs), autonomous driving systems (ADS) have achieved significant development and been applied to assist in many safety-critical tasks. However, despite their spectacular progress, several real-world accidents involving autonomous cars even resulted in a fatality. While the high complexity and low interpretability of DNN models, which empowers the perception capability of ADS, make conventional testing techniques inapplicable for the perception of ADS, the existing testing techniques depending on manual data collection and labeling become time-consuming and prohibitively expensive.  

  
In this paper, we design and implement LiRTest, the first automated LiDAR-based autonomous vehicles testing tool. LiRTest implements the ADS-specific metamorphic relation and equips affine and weather transformation operators that can reflect the impact of the various environmental factors to implement the relation. We experiment LiRTest with multiple 3D object detection models to evaluate its performance on different tasks. The experiment results show that LiRTest can activate different neurons of the object detection models and effectively detect their erroneous behaviors under various driving conditions. Also, the results confirm that LiRTest can improve the object detection precision by retraining with the generated data.},
booktitle = {Proceedings of the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {480–492},
numpages = {13},
keywords = {Autonomous Driving System, Data Augmentation, Metamorphic Testing, Software Testing},
location = {<conf-loc>, <city>Virtual</city>, <country>South Korea</country>, </conf-loc>},
series = {ISSTA 2022}
}

@inproceedings{10.1145/3377930.3389827,
author = {Cal\`{o}, Alessandro and Arcaini, Paolo and Ali, Shaukat and Hauer, Florian and Ishikawa, Fuyuki},
title = {Simultaneously searching and solving multiple avoidable collisions for testing autonomous driving systems},
year = {2020},
isbn = {9781450371285},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377930.3389827},
doi = {10.1145/3377930.3389827},
abstract = {The oracle problem is a key issue in testing Autonomous Driving Systems (ADS): when a collision is found, it is not always clear whether the ADS is responsible for it. Our recent search-based testing approach offers a solution to this problem by defining a collision as avoidable if a differently configured ADS would have avoided it. This approach searches for both collision scenarios and the ADS configurations capable of avoiding them. However, its main problem is that the ADS configurations generated for avoiding some collisions are not suitable for preventing other ones. Therefore, it does not provide any guidance to automotive engineers for improving the safety of the ADS. To this end, we propose a new search-based approach to generate configurations of the ADS that can avoid as many different types of collisions as possible. We present two versions of the approach, which differ in the way of searching for collisions and alternative configurations. The approaches have been experimented on the path planner component of an ADS provided by our industry partner.},
booktitle = {Proceedings of the 2020 Genetic and Evolutionary Computation Conference},
pages = {1055–1063},
numpages = {9},
keywords = {autonomous driving, avoidable collision, path planner, search-based testing},
location = {Canc\'{u}n, Mexico},
series = {GECCO '20}
}

@inproceedings{10.1145/3597503.3623350,
author = {Dai, Jiarun and Gao, Bufan and Luo, Mingyuan and Huang, Zongan and Li, Zhongrui and Zhang, Yuan and Yang, Min},
title = {SCTrans: Constructing a Large Public Scenario Dataset for Simulation Testing of Autonomous Driving Systems},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3623350},
doi = {10.1145/3597503.3623350},
abstract = {For the safety assessment of autonomous driving systems (ADS), simulation testing has become an important complementary technique to physical road testing. In essence, simulation testing is a scenario-driven approach, whose effectiveness is highly dependent on the quality of given simulation scenarios. Moreover, simulation scenarios should be encoded into well-formatted files, otherwise, ADS simulation platforms cannot take them as inputs. Without large public datasets of simulation scenario files, both industry and academic applications of ADS simulation testing are hindered.To fill this gap, we propose a transformation-based approach SCTrans to construct simulation scenario files, utilizing existing traffic scenario datasets (i.e., naturalistic movement of road users recorded on public roads) as data sources. Specifically, we try to transform existing traffic scenario recording files into simulation scenario files that are compatible with the most advanced ADS simulation platforms, and this task is formalized as a Model Transformation Problem. Following this idea, we construct a dataset consisting of over 1,900 diverse simulation scenarios, each of which can be directly used to test the state-of-the-art ADSs (i.e., Apollo and Autoware) via high-fidelity simulators (i.e., Carla and LGSVL). To further demonstrate the utility of our dataset, we showcase that it can boost the collision-finding capability of existing simulation-based ADS fuzzers, helping identify about seven times more unique ADS-involved collisions within the same time period. By analyzing these collisions at the code level, we identify nine safety-critical bugs of Apollo and Autoware, each of which can be stably exploited to cause vehicle crashes. Till now, four of them have been confirmed.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {50},
numpages = {13},
keywords = {simulation scenario, autonomous driving, model transformation},
location = {<conf-loc>, <city>Lisbon</city>, <country>Portugal</country>, </conf-loc>},
series = {ICSE '24}
}

@inproceedings{10.1145/3540250.3549111,
author = {Lou, Guannan and Deng, Yao and Zheng, Xi and Zhang, Mengshi and Zhang, Tianyi},
title = {Testing of autonomous driving systems: where are we and where should we go?},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3549111},
doi = {10.1145/3540250.3549111},
abstract = {Autonomous driving has shown great potential to reform modern transportation. Yet its reliability and safety have drawn a lot of attention and concerns. Compared with traditional software systems, autonomous driving systems (ADSs) often use deep neural networks in tandem with logic-based modules. This new paradigm poses unique challenges for software testing. Despite the recent development of new ADS testing techniques, it is not clear to what extent those techniques have addressed the needs of ADS practitioners. To fill this gap, we present the first comprehensive study to identify the current practices and needs of ADS testing. We conducted semi-structured interviews with developers from 10 autonomous driving companies and surveyed 100 developers who have worked on autonomous driving systems. A systematic analysis of the interview and survey data revealed 7 common practices and 4 emerging needs of autonomous driving testing. Through a comprehensive literature review, we developed a taxonomy of existing ADS testing techniques and analyzed the gap between ADS research and practitioners’ needs. Finally, we proposed several future directions for SE researchers, such as developing test reduction techniques to accelerate simulation-based ADS testing.},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {31–43},
numpages = {13},
keywords = {Autonomous Driving, Empirical Study, Software Testing},
location = {<conf-loc>, <city>Singapore</city>, <country>Singapore</country>, </conf-loc>},
series = {ESEC/FSE 2022}
}

@inproceedings{10.1145/3388767.3408334,
author = {Kaviratna, Neal},
title = {Testing Self Driving Cars with Game Development Tools},
year = {2020},
isbn = {9781450379717},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3388767.3408334},
doi = {10.1145/3388767.3408334},
abstract = {The Uber Advanced Technologies Group (ATG) aims to bring safe, reliable self‑driving transportation to everyone, everywhere; In order to facilitate this mission, a suite of simulation technologies for exercising the self driving vehicle (SDV) needs to be created. These simulations must be scale-able, believable, and human interpretable in order to provide consistent value. Here we will cover how ATG leveraged a game engine to meet these demands. Topics covered include: evaluating self driving performance, authoring self driving tests, simulating robot-realistic worlds, and critical lessons learned while building these tools.},
booktitle = {ACM SIGGRAPH 2020 Talks},
articleno = {54},
numpages = {2},
keywords = {autonomy, game dev, robotics, self driving cars, simulation},
location = {Virtual Event, USA},
series = {SIGGRAPH '20}
}

@article{10.1145/3533818,
author = {Birchler, Christian and Khatiri, Sajad and Derakhshanfar, Pouria and Panichella, Sebastiano and Panichella, Annibale},
title = {Single and Multi-objective Test Cases Prioritization for Self-driving Cars in Virtual Environments},
year = {2023},
issue_date = {March 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3533818},
doi = {10.1145/3533818},
abstract = {Testing with simulation environments helps to identify critical failing scenarios for self-driving cars (SDCs). Simulation-based tests are safer than in-field operational tests and allow detecting software defects before deployment. However, these tests are very expensive and are too many to be run frequently within limited time constraints.In this article, we investigate test case prioritization techniques to increase the ability to detect SDC regression faults with virtual tests earlier. Our approach, called SDC-Prioritizer, prioritizes virtual tests for SDCs according to static features of the roads we designed to be used within the driving scenarios. These features can be collected without running the tests, which means that they do not require past execution results. We introduce two evolutionary approaches to prioritize the test cases using diversity metrics (black-box heuristics) computed on these static features. These two approaches, called SO-SDC-Prioritizer and MO-SDC-Prioritizer, use single-objective and multi-objective genetic algorithms (GA), respectively, to find trade-offs between executing the less expensive tests and the most diverse test cases earlier.Our empirical study conducted in the SDC domain shows that MO-SDC-Prioritizer significantly (P- value &lt;=0.1e-10) improves the ability to detect safety-critical failures at the same level of execution time compared to baselines: random and greedy-based test case orderings. Besides, our study indicates that multi-objective meta-heuristics outperform single-objective approaches when prioritizing simulation-based tests for SDCs.MO-SDC-Prioritizer prioritizes test cases with a large improvement in fault detection while its overhead (up to 0.45% of the test execution cost) is negligible.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {apr},
articleno = {28},
numpages = {30},
keywords = {Autonomous systems, software simulation, test case prioritization}
}

@inproceedings{10.1145/3377811.3380422,
author = {Zhou, Husheng and Li, Wei and Kong, Zelun and Guo, Junfeng and Zhang, Yuqun and Yu, Bei and Zhang, Lingming and Liu, Cong},
title = {DeepBillboard: systematic physical-world testing of autonomous driving systems},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380422},
doi = {10.1145/3377811.3380422},
abstract = {Deep Neural Networks (DNNs) have been widely applied in autonomous systems such as self-driving vehicles. Recently, DNN testing has been intensively studied to automatically generate adversarial examples, which inject small-magnitude perturbations into inputs to test DNNs under extreme situations. While existing testing techniques prove to be effective, particularly for autonomous driving, they mostly focus on generating digital adversarial perturbations, e.g., changing image pixels, which may never happen in the physical world. Thus, there is a critical missing piece in the literature on autonomous driving testing: understanding and exploiting both digital and physical adversarial perturbation generation for impacting steering decisions. In this paper, we propose a systematic physical-world testing approach, namely DeepBillboard, targeting at a quite common and practical driving scenario: drive-by billboards. DeepBillboard is capable of generating a robust and resilient printable adversarial billboard test, which works under dynamic changing driving conditions including viewing angle, distance, and lighting. The objective is to maximize the possibility, degree, and duration of the steering-angle errors of an autonomous vehicle driving by our generated adversarial billboard. We have extensively evaluated the efficacy and robustness of DeepBillboard by conducting both experiments with digital perturbations and physical-world case studies. The digital experimental results show that DeepBillboard is effective for various steering models and scenes. Furthermore, the physical case studies demonstrate that DeepBillboard is sufficiently robust and resilient for generating physical-world adversarial billboard tests for real-world driving under various weather conditions, being able to mislead the average steering angle error up to 26.44 degrees. To the best of our knowledge, this is the first study demonstrating the possibility of generating realistic and continuous physical-world tests for practical autonomous driving systems; moreover, DeepBillboard can be directly generalized to a variety of other physical entities/surfaces along the curbside, e.g., a graffiti painted on a wall.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {347–358},
numpages = {12},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1109/MODELS-C.2019.00009,
author = {Wiecher, Carsten and Greenyer, Joel and Korte, Jan},
title = {Test-driven scenario specification of automotive software components},
year = {2021},
isbn = {9781728151250},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MODELS-C.2019.00009},
doi = {10.1109/MODELS-C.2019.00009},
abstract = {The rising complexity of automotive software makes it increasingly difficult to develop the software with high quality in short time. Especially the late detection of early errors, such as requirement inconsistencies and ambiguities, often causes costly iterations. We address this problem with a new requirements specification and analysis technique based on executable scenarios and automated testing. The technique is based on the Scenario Modeling Language for Kotlin (SMLK), a Kotlin-based framework that supports the modeling/programming of behavior as loosely coupled scenarios, which is close to how humans conceive and communicate behavioral requirements. Combined with JUnit, we propose the Test-Driven Scenario Specification (TDSS) process, which introduces agile practices into the early phases of development, significantly reducing the risk of requirement inconsistencies and ambiguities, and, thus, reducing development costs. We overview TDSS with the help of an example from the e-mobility domain, report on lessons learned, and outline open challenges.},
booktitle = {Proceedings of the 22nd International Conference on Model Driven Engineering Languages and Systems},
pages = {12–17},
numpages = {6},
keywords = {automotive software engineering, requirements analysis, software development, software test, test-driven development},
location = {Munich, Germany},
series = {MODELS '19}
}

@inproceedings{10.1109/ICSE48619.2023.00216,
author = {Huai, Yuqi and Chen, Yuntianyi and Almanee, Sumaya and Ngo, Tuan and Liao, Xiang and Wan, Ziwen and Chen, Qi Alfred and Garcia, Joshua},
title = {Doppelg\"{a}nger Test Generation for Revealing Bugs in Autonomous Driving Software},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00216},
doi = {10.1109/ICSE48619.2023.00216},
abstract = {Vehicles controlled by autonomous driving software (ADS) are expected to bring many social and economic benefits, but at the current stage not being broadly used due to concerns with regard to their safety. Virtual tests, where autonomous vehicles are tested in software simulation, are common practices because they are more efficient and safer compared to field operational tests. Specifically, search-based approaches are used to find particularly critical situations. These approaches provide an opportunity to automatically generate tests; however, systematically producing bug-revealing tests for ADS remains a major challenge. To address this challenge, we introduce DoppelTest, a test generation approach for ADSes that utilizes a genetic algorithm to discover bug-revealing violations by generating scenarios with multiple autonomous vehicles that account for traffic control (e.g., traffic signals and stop signs). Our extensive evaluation shows that DoppelTest can efficiently discover 123 bug-revealing violations for a production-grade ADS (Baidu Apollo) which we then classify into 8 unique bug categories.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2591–2603},
numpages = {13},
keywords = {cyber-physical systems, autonomous driving systems, search-based software testing},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3597926.3598070,
author = {Zhang, Xudong and Cai, Yan},
title = {Building Critical Testing Scenarios for Autonomous Driving from Real Accidents},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3598070},
doi = {10.1145/3597926.3598070},
abstract = {One of the aims of the development and spread of autonomous driving technology is to reduce traffic accidents caused by human factors.  
But recently reported data on fatal accidents involving autonomous driving system (ADS) shows that this important goal has not been achieved.  
So there is an emerge requirement on more comprehensive and targeted testing especially on safe driving.  
In this paper, we propose an approach to automatically building critical testing scenarios from real-world accident data.  
Firstly, we propose a new model called M-CPS (Multi-channel Panoptic Segmentation) to extract the effective information from the accident record (such as images or videos), and separate the independent individuals of different traffic participants for further scene recovery.  
Compared with the traditional panoramic segmentation models, M-CPS model is able to effectively handle segmentation challenges due to the shooting angle, image quality, pixel overlap and other problems existing in the accident record.  
Next, the extracted core information is then connected with the virtual testing platform to generate the original scene set.  
Besides, we also design a mutation testing solution on the basis of the original scene set, thus greatly enriching the scene library for testing.  
In our experiments,  
the M-CPS model reaches a result of 66.1% PQ on CityScapes test set, shows that our model has only slight fluctuations on performance compared with the best benchmark model on pure panoptic segmentation task.  
It also reaches a result of 84.5% IoU for semantic segmentation branch and 40.3% mAP for instance segmentation branch on SHIFT dataset.  
Then we use UCF-Crime, CADP and US-Accidents datasets to generate the original and mutated scene set.  
Those generated scene sets are connected to Apollo and Carla simulation platforms to test ADS prototypes.  
We find three types of scenarios that can lead to accidents of ADS prototypes, which indicates that the existing ADS prototype has defects.  
Our solution provides a new possible direction for the recovery of key scenarios in ADS testing, and can improve the efficiency in related fields.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {462–474},
numpages = {13},
keywords = {Autonomous Driving Systems, Panoptic Segmentation, Parameter Mutation, Scene Recovery, Testing},
location = {<conf-loc>, <city>Seattle</city>, <state>WA</state>, <country>USA</country>, </conf-loc>},
series = {ISSTA 2023}
}

@inproceedings{10.1145/3579109.3579127,
author = {Ding, Yueran and Zou, Junhong and Fan, Yixuan and Wang, Shengjin and Liao, Qingmin},
title = {A Digital Twin-based Testing and Data Collection System for Autonomous Driving in Extreme Traffic Scenarios},
year = {2023},
isbn = {9781450397568},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579109.3579127},
doi = {10.1145/3579109.3579127},
abstract = {Autonomous driving systems need to undergo rigorous testing in complex scenarios including a variety of extreme operating conditions before they can be put into use. In this process, digital twin technology can migrate the scenes in the physical world to computer simulation software, so that engineers can comprehensively and safely conduct large-scale test experiments, get well prepared for vehicle tests on real road. The existing related systems in academia are still lacking in the authenticity, diversity and complexity of the scene. To solve the above problems, we propose a digital twin-based test and data acquisition system for autonomous driving under extreme traffic scenes. Based on UE4 engine and CARLA simulation platform, the system includes a Chinese urban style map with a total area of 10.8 square kilometers and 232 multi-site test routes including 29 possible events, 4 sets of environmental parameters, 2 sets of location parameters, and all weather condition. At the same time, a large amount of sensor data can also be collected in the system, which fills the insufficiency of collecting extreme working scene data in the real world. We give a perception-oriented autonomous driving data collection scheme, which can store sensor output while running simulation events, and automatically generate corresponding annotations, which can collect data in emergency situations. We also propose a new evaluation metric of autonomous driving system based on research on accident hazard. This test system covers a variety of extreme working conditions that are not involved in the existing systems, and puts forward higher requirements for the perception and decision-making algorithms related to autonomous driving.},
booktitle = {Proceedings of the 2022 6th International Conference on Video and Image Processing},
pages = {101–109},
numpages = {9},
keywords = {Autonomous Driving, Benchmark, Datasets, Scenarios Simulation},
location = {Shanghai, China},
series = {ICVIP '22}
}

@inproceedings{10.1145/3459104.3459208,
author = {Brade, Tino and Kramer, Birte and Neurohr, Christian},
title = {Paradigms in Scenario-Based Testing for Automated Driving},
year = {2021},
isbn = {9781450389839},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3459104.3459208},
doi = {10.1145/3459104.3459208},
abstract = {Scenario-based testing is at present regarded as the enabling tool for the homologation of automated driving (AD) systems (SAE Level ≥ 3). This is mainly due to the aspiration of scenario-based testing for gaining understanding about the tested AD system, which provides support for arguments that shall convince ourselves about its capabilities. Since this way of testing involves system-dependent intertwinings, inconsistencies can thus impair the correctness of the homologation. In order to avoid this, we propose encapsulating these intertwinings by means of paradigms, which are individual mindsets towards homologation. For their definition, we introduce behaviors as a respective language, in combination with behavioral shifts, for the systematic derivation of what needs to be defined for a paradigm. The applicability is shown by mapping the derived paradigms against proposed approaches in automated driving. Furthermore, this systematics helps to identify concerns about using paradigms in scenario-based testing.},
booktitle = {2021 International Symposium on Electrical, Electronics and Information Engineering},
pages = {108–114},
numpages = {7},
keywords = {Automated Driving, Paradigms, Scenario-Based Testing},
location = {Seoul, Republic of Korea},
series = {ISEEIE 2021}
}

@inproceedings{10.1145/3449726.3462723,
author = {Laurent, Thomas and Arcaini, Paolo and Ishikawa, Fuyuki and Ventresque, Anthony},
title = {Achieving weight coverage for an autonomous driving system with search-based test generation (HOP track at GECCO 2021)},
year = {2021},
isbn = {9781450383516},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3449726.3462723},
doi = {10.1145/3449726.3462723},
abstract = {Autonomous Driving Systems (ADSs) are complex critical systems that must be thoroughly tested. Still, assessing the strength of tests for ADSs is an open and complex problem. Weight Coverage is a test criterion targeting ADSs which are based on a weighted cost function. It measures how much each weight, related to different aspects of the ADS's decision process, is involved in the decisions taken in a test scenario. All weights/aspects should be involved for a strong test suite. Although weight coverage can measure the quality of a test suite, it does not provide clear guidance for test generation. This work proposes weight coverage-driven search-based test generation for ADSs. It describes and compares three designs of the search process: a single-objective search aiming at generating a test covering a single weight; a multi-objective search where each objective targets a different weight; and a single-objective search where the fitness function is an aggregate function representing the coverage over multiple weights. Experiments using an ADS system provided by our industry partner show the validity of the method and provide insights into the benefits of each search design. This Hot-off-the-Press paper summarises the paper [2]: T. Laurent, P. Arcaini, F. Ishikawa and A. Ventresque, "Achieving Weight Coverage for an Autonomous Driving System with Search-based Test Generation", 25th International Conference on Engineering of Complex Computer Systems (ICECCS 2020).},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {33–34},
numpages = {2},
keywords = {autonomous driving, search-based testing, weight coverage},
location = {Lille, France},
series = {GECCO '21}
}

@inproceedings{10.1145/3507959.3507968,
author = {Zhang, Ziqi and Wang, Weixiang and Zhong, Haozhe and Wang, Lang},
title = {Autonomous Driving's Testing Map Generating System},
year = {2022},
isbn = {9781450388009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3507959.3507968},
doi = {10.1145/3507959.3507968},
abstract = {In the twenty-first Century, AI and digital technology have quickly developed to a high level, bringing the application of automatic control into reality. In this research paper, we are going to present our motivation and challenge while creating the “Autonomous Driving's Testing Map Generating System”. Next, we are going to introduce the concepts and approaches used to achieve the goal, like “Bezier Curve”. After that, there will be evaluation methods and metrics provided. Moreover, we are going to show our results by both word statements and python's pictures. Also, we will discuss our results and plan for future improvements or editions. Finally, the detailed task allocations among we team members will be introduced.},
booktitle = {Proceedings of the 3rd International Conference on Intelligent Science and Technology},
pages = {50–54},
numpages = {5},
keywords = {Auto-generated map, Autonomous driving, Bezier Curve, Python},
location = {Tokyo, Japan},
series = {ICIST '21}
}

@inproceedings{10.1145/3462741.3466655,
author = {Pathrudkar, Sagar and Mukherjee, Saikat and Sarathi, Vijaya and Chowdhary, Manish},
title = {SceVar (Scenario Variations) Database: Real World Statistics driven Scenario Variations for AV Testing in Simulation: Abstraction of static and dynamic entities from road network-traffic ecosystem and their interactions or relationships in semantic data models for realistic simulation-based testing of AVs},
year = {2021},
isbn = {9781450385251},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3462741.3466655},
doi = {10.1145/3462741.3466655},
abstract = {Autonomous vehicles (AV) are on a journey from incubation to a widespread usage. A major challenge for self-driving cars to reach widespread usage are guarantees about their safety and reliability. One approach to increasing safety and reliability is testing in simulation. However, simulation-based testing can be beneficial only if the simulations mimic the real-world phenomena. Additionally, AV environments are characterized by high-dimensionality, nonlinearity, stochasticity, and non-stationarity, hence it is difficult in practice to exhaustively list and test all possible scenario variations. While this is a typical state space explosion problem, as usual it is essential to remove un-realistic scenarios from the test space, to reach a more realistic and plausible list of scenario variations. In this work, we present SceVar – a scenario variations database which analyzes real world driving data to extract realistic traffic patterns and driving behaviors which is then used to create massive number of scenario variations for (regression) testing in simulation. In addition, we also envisage that such statistical data can also be used by AV regulatory and testing agencies to certify vehicles for usage in specific operational design domains (ODDs).},
booktitle = {Companion Publication of the 13th ACM Web Science Conference 2021},
pages = {126–129},
numpages = {4},
keywords = {Semantic Data Models, Simulation-based Testing of Autonomous Vehicles, Statistical Analysis, Traffic Behavior Modelling},
location = {<conf-loc>, <city>Virtual Event</city>, <country>United Kingdom</country>, </conf-loc>},
series = {WebSci '21 Companion}
}

@inproceedings{10.1145/3551349.3559528,
author = {Wang, Sen and Sheng, Zhuheng and Xu, Jingwei and Chen, Taolue and Zhu, Junjun and Zhang, Shuhui and Yao, Yuan and Ma, Xiaoxing},
title = {ADEPT: A Testing Platform for Simulated Autonomous Driving},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3559528},
doi = {10.1145/3551349.3559528},
abstract = {Effective quality assurance methods for autonomous driving systems ADS have attracted growing interests recently. In this paper, we report a new testing platform ADEPT, aiming to provide practically realistic and comprehensive testing facilities for DNN-based ADS. ADEPT is based on the virtual simulator CARLA and provides numerous testing facilities such as scene construction, ADS importation, test execution and recording, etc. In particular, ADEPT features two distinguished test scenario generation strategies designed for autonomous driving. First, we make use of real-life accident reports from which we leverage natural language processing to fabricate abundant driving scenarios. Second, we synthesize physically-robust adversarial attacks by taking the feedback of ADS into consideration and thus are able to generate closed-loop test scenarios. The experiments confirm the efficacy of the platform.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {150},
numpages = {4},
keywords = {Autonomous driving, Deep neural networks, Software testing, Test case generation, Testing platform},
location = {<conf-loc>, <city>Rochester</city>, <state>MI</state>, <country>USA</country>, </conf-loc>},
series = {ASE '22}
}

@inproceedings{10.1145/3568160.3570235,
author = {Malayjerdi, Mohsen and Roberts, Andrew and Maennel, Olaf manuel and Malayjerdi, Ehsan},
title = {Combined Safety and Cybersecurity Testing Methodology for Autonomous Driving Algorithms},
year = {2022},
isbn = {9781450397865},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3568160.3570235},
doi = {10.1145/3568160.3570235},
abstract = {Combined safety and cybersecurity testing are critical for assessing the reliability and optimisation of autonomous driving (AD) algorithms. However, safety and cybersecurity testing is often conducted in isolation, leading to a lack of evaluation of the complex system-of-system interactions which impact the reliability and optimisation of the AD algorithm. Concurrently, practical limitations of testing include resource usage and time. This paper proposes a methodology for combined safety and cybersecurity testing and applies it to a real-world AV shuttle using digital twin, software-in-the-loop (SiL) simulation and a real-world Autonomous Vehicle (AV) test environment. The results of the safety and cybersecurity tests and feedback from the AD algorithm designers demonstrate that the methodology developed is useful for assessing the reliability and optimisation of an AD algorithm in the development phase. Furthermore, from the observed system-of-system interactions, key relationships such as speed and attack parameters can be used to optimise testing.},
booktitle = {Proceedings of the 6th ACM Computer Science in Cars Symposium},
articleno = {12},
numpages = {10},
keywords = {automotive cybersecurity, autonomous driving, safety testing},
location = {Ingolstadt, Germany},
series = {CSCS '22}
}

@inproceedings{10.1145/3558819.3565100,
author = {Yuan, Wenjing and Zhang, Li and Wang, Rui and Du, Baocheng and Xu, Hualong and Zhang, Xiaoqi and Qi, Guoli and Li, Yang},
title = {Study on Carbon Emission Intelligent Test System and Evaluation Model of Hybrid Electric Vehicle in Actual Road Driving},
year = {2022},
isbn = {9781450397414},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3558819.3565100},
doi = {10.1145/3558819.3565100},
abstract = {In order to reflect the carbon emission characteristics of HEV under actual driving conditions, and the influence of driving behavior, roads and other factors on its actual driving carbon emission characteristics, a HEV model with good market sales in China market was selected, and its actual driving carbon emission performance was tested on the same test line as a gasoline vehicle with similar structure type, and the data window was divided by moving average window method to analyze the carbon emission characteristics of HEV and its reference counterpart (conventional gasoline vehicle) under actual driving conditions. The results show that HEV has obvious low CO2 emission characteristics when driving on actual roads, especially in urban and suburban sections. In addition, the actual driving carbon emissions of the test vehicle are related to the driving route topography and the dynamic characteristics of the trip. Overall, the CO2 emission factor shows an overall trend of increasing with the increase of RPA, but this positive correlation is obviously weaker than the positive correlation between CO2 emission factor and cumulative positive altitude increment. However, the carbon emission characteristics of HEV are unique. In urban and suburban driving conditions, the CO2 emission factor of HEV has no obvious correlation with the cumulative positive altitude increment. And the CO2 emission factor decreases with the increase of RPA, showing a strong negative correlation.},
booktitle = {Proceedings of the 7th International Conference on Cyber Security and Information Engineering},
pages = {317–323},
numpages = {7},
location = {<conf-loc>, <city>Brisbane</city>, <state>QLD</state>, <country>Australia</country>, </conf-loc>},
series = {ICCSIE '22}
}

@inproceedings{10.1145/3409118.3475128,
author = {Nagaraju, Divyabharathi and Ansah, Alberta and Ch, Nabil Al Nahin and Mills, Caitlin and Janssen, Christian P. and Shaer, Orit and Kun, Andrew L},
title = {How Will Drivers Take Back Control in Automated Vehicles? A Driving Simulator Test of an Interleaving Framework},
year = {2021},
isbn = {9781450380638},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3409118.3475128},
doi = {10.1145/3409118.3475128},
abstract = {We explore the transfer of control from an automated vehicle to the driver. Based on data from N=19 participants who participated in a driving simulator experiment, we find evidence that the transfer of control often does not take place in one step. In other words, when the automated system requests the transfer of control back to the driver, the driver often does not simply stop the non-driving task. Rather, the transfer unfolds as a process of interleaving the non-driving and driving tasks. We also find that the process is moderated by the length of time available for the transfer of control: interleaving is more likely when more time is available. Our interface designs for automated vehicles must take these results into account so as to allow drivers to safely take back control from automation.},
booktitle = {13th International Conference on Automotive User Interfaces and Interactive Vehicular Applications},
pages = {20–27},
numpages = {8},
keywords = {automated driving, interleaving framework, transfer of control},
location = {Leeds, United Kingdom},
series = {AutomotiveUI '21}
}

@inproceedings{10.1145/3544549.3585659,
author = {Kretzer, Felix and Maedche, Alexander},
title = {Making Usability Test Data Actionable! A Quantitative Test-Driven Prototyping Approach},
year = {2023},
isbn = {9781450394222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544549.3585659},
doi = {10.1145/3544549.3585659},
abstract = {In recent years, the availability and use of prototyping and usability testing tools has increased massively in practice. In parallel, research introduced multiple novel approaches and tools to assist novice and expert designers in prototyping. However, existing research has not looked into how to assist designers in conducting usability tests and making the collected quantitative data directly available in prototyping tools. In this paper, we distill three design goals from literature and propose a novel quantitative test-driven prototyping approach. The approach in its current form consists of a process and two user interface artifacts supporting selected actions of our process. We conducted a formative study to gather feedback from potential users on our approach. We use the collected feedback to refine our approach, identify trade-offs, and propose future research directions. We contribute by providing a novel quantitative test-driving prototyping approach that tightly connects prototyping with asynchronous usability testing.},
booktitle = {Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {200},
numpages = {6},
keywords = {Data-Driven, GUI, Prototyping, Usability Testing},
location = {<conf-loc>, <city>Hamburg</city>, <country>Germany</country>, </conf-loc>},
series = {CHI EA '23}
}

@inproceedings{10.1145/3468264.3473916,
author = {Mehta, Sonu and Farmahinifarahani, Farima and Bhagwan, Ranjita and Guptha, Suraj and Jafari, Sina and Kumar, Rahul and Saini, Vaibhav and Santhiar, Anirudh},
title = {Data-driven test selection at scale},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3473916},
doi = {10.1145/3468264.3473916},
abstract = {Large-scale services depend on Continuous Integration/Continuous Deployment (CI/CD) processes to maintain their agility and code-quality. Change-based testing plays an important role in finding bugs, but testing after every change is prohibitively expensive at a scale where thousands of changes are committed every hour. Test selection models deal with this issue by running a subset of tests for every change. In this paper, we present a generic, language-agnostic and lightweight statistical model for test selection. Unlike existing techniques, the proposed model does not require complex feature extraction techniques. Consequently, it scales to hundreds of repositories of varying characteristics while capturing more than 99% of buggy pull requests. Additionally, to better evaluate test selection models, we propose application-specific metrics that capture both a reduction in resource cost and a reduction in pull-request turn-around time. By evaluating our model on 22 large repositories at Microsoft, we find that we can save 15%−30% of compute time while reporting back more than ≈99% of buggy pull requests.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1225–1235},
numpages = {11},
keywords = {continuous integration, statistical models, test selection},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@inproceedings{10.1145/3510466.3510474,
author = {Birkemeyer, Lukas and Pett, Tobias and Vogelsang, Andreas and Seidl, Christoph and Schaefer, Ina},
title = {Feature-Interaction Sampling for Scenario-based Testing of Advanced Driver Assistance Systems✱},
year = {2022},
isbn = {9781450396042},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510466.3510474},
doi = {10.1145/3510466.3510474},
abstract = {Scenario-based testing is considered state-of-the-art to verify and validate Advanced Driver Assistance Systems. However, two essential unsolved challenges prevent the practical application of scenario-based testing according to the SOTIF-standard: (1)&nbsp;how to select a set of representative test scenarios, and (2)&nbsp;how to assess the effectiveness of a test scenario suite. In this paper, we leverage variability modelling techniques to select scenarios from a scenario space and assess the resulting scenario suites with a mutation score as metric. We capture the scenario space in a feature model and generate representative subsets with feature-interaction coverage sampling. The mutation score assesses the failure-finding effectiveness of these samples. We evaluate our concepts by sampling scenario suites for two independent Autonomous Emergency Braking function implementations and executing them on an industrial-strength simulator. Our results show that the feature model captures a scenario space that is relevant to identify all mutants. We show that sampling based on interaction coverage reduces the testing effort significantly while maintaining effectiveness in terms of mutation scores. Our results underline the potential of feature model sampling for testing in the automotive industry.},
booktitle = {Proceedings of the 16th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {5},
numpages = {10},
keywords = {ADAS, SOTIF, Sampling Strategies, Scenario-based testing},
location = {Florence, Italy},
series = {VaMoS '22}
}

@inproceedings{10.1145/3571788.3571794,
author = {Eichhorn, Domenik and Pett, Tobias and Przigoda, Nils and Kindsvater, Jessica and Seidl, Christoph and Schaefer, Ina},
title = {Coverage-Driven Test Automation for Highly-Configurable Railway Systems},
year = {2023},
isbn = {9798400700019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3571788.3571794},
doi = {10.1145/3571788.3571794},
abstract = {Railway track systems contain nested variability, consisting of various configurable elements interacting in a configurable environment. Current test strategies rely on integration tests that simulate realistic operational scenarios defined by experts. Due to the increasing size and high configurability of railway software, the sole reliance on such test strategies is no longer feasible as the effort to reach test coverage increases exponentially. As a result, developers have to rely on slow testing processes that increase the development times of railway systems. In this paper, we leverage variability engineering techniques to create a procedure for automated, coverage-driven unit testing of railway systems. We generate sets of test cases using static code analysis on 150% models of track elements and propose different ways for automated test case execution in simulated test environments. We implemented our concept in a tool and applied it to railway projects provided by an industry partner. Our evaluation shows that we can automatically derive 150% models and compute test sets achieving branch coverage in a few seconds. Additionally, we show that an automated execution and evaluation of our test cases is possible in about 10 to 30 minutes, which is a significant speedup compared to current test strategies.},
booktitle = {Proceedings of the 17th International Working Conference on Variability Modelling of Software-Intensive Systems},
pages = {23–30},
numpages = {8},
keywords = {Model-based Testing, Railway Systems, Variability Engineering},
location = {<conf-loc>, <city>Odense</city>, <country>Denmark</country>, </conf-loc>},
series = {VaMoS '23}
}

@inproceedings{10.1145/3581754.3584133,
author = {Figalov\'{a}, Nikol and Bieg, Hans-Joachim and Schulz, Michael and Pichen, J\"{u}rgen and Baumann, Martin and Chuang, Lewis L and Pollatos, Olga},
title = {Fatigue and mental underload further pronounced in L3 conditionally automated driving: Results from an EEG experiment on a test track},
year = {2023},
isbn = {9798400701078},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3581754.3584133},
doi = {10.1145/3581754.3584133},
abstract = {Drivers’ role changes with increasing automation from the primary driver to a system supervisor. This study investigates how supervising an SAE L2 and L3 automated vehicle (AV) affects drivers’ mental workload and sleepiness compared to manual driving. Using an AV prototype on a test track, the oscillatory brain activity of 23 adult participants was recorded during L2, L3, and manual driving. Results showed decreased mental workload and increased sleepiness in L3 drives compared to L2 and manual drives, indicated by self-report scales and changes in the frontal alpha and theta power spectral density. These findings suggest that fatigue and mental underload are significant issues in L3 driving and should be considered when designing future AV interfaces.},
booktitle = {Companion Proceedings of the 28th International Conference on Intelligent User Interfaces},
pages = {64–67},
numpages = {4},
keywords = {EEG, L3 driving, automated driving, conditional automation, drowsiness, fatigue, mental underload},
location = {<conf-loc>, <city>Sydney</city>, <state>NSW</state>, <country>Australia</country>, </conf-loc>},
series = {IUI '23 Companion}
}

@inproceedings{10.1145/3510003.3510059,
author = {Zhao, Yingquan and Wang, Zan and Chen, Junjie and Liu, Mengdi and Wu, Mingyuan and Zhang, Yuqun and Zhang, Lingming},
title = {History-driven test program synthesis for JVM testing},
year = {2022},
isbn = {9781450392211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3510003.3510059},
doi = {10.1145/3510003.3510059},
abstract = {Java Virtual Machine (JVM) provides the runtime environment for Java programs, which allows Java to be "write once, run anywhere". JVM plays a decisive role in the correctness of all Java programs running on it. Therefore, ensuring the correctness and robustness of JVM implementations is essential for Java programs. To date, various techniques have been proposed to expose JVM bugs via generating potential bug-revealing test programs. However, the diversity and effectiveness of test programs generated by existing research are far from enough since they mainly focus on minor syntactic/semantic mutations. In this paper, we propose JavaTailor, the first history-driven test program synthesis technique, which synthesizes diverse test programs by weaving the ingredients extracted from JVM historical bug-revealing test programs into seed programs for covering more JVM behaviors/paths. More specifically, JavaTailor first extracts five types of code ingredients from the historical bug-revealing test programs. Then, to synthesize diverse test programs, it iteratively inserts the extracted ingredients into the seed programs and strengthens their interactions via introducing extra data dependencies between them. Finally, JavaTailor employs these synthesized test programs to differentially test JVMs. Our experimental results on popular JVM implementations (i.e., HotSpot and OpenJ9) show that JavaTailor outperforms the state-of-the-art technique in generating more diverse and effective test programs, e.g., test programs generated by JavaTailor can achieve higher JVM code coverage and detect many more unique inconsistencies than the state-of-the-art technique. Furthermore, JavaTailor has detected 10 previously unknown bugs, 6 of which have been confirmed/fixed by developers.},
booktitle = {Proceedings of the 44th International Conference on Software Engineering},
pages = {1133–1144},
numpages = {12},
keywords = {JVM testing, Java virtual machine, compiler testing, program synthesis},
location = {Pittsburgh, Pennsylvania},
series = {ICSE '22}
}

@inproceedings{10.1145/3597926.3598108,
author = {Zhang, Xiaodong and Zhao, Wei and Sun, Yang and Sun, Jun and Shen, Yulong and Dong, Xuewen and Yang, Zijiang},
title = {Testing Automated Driving Systems by Breaking Many Laws Efficiently},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3598108},
doi = {10.1145/3597926.3598108},
abstract = {An automated driving system (ADS), as the brain of an autonomous vehicle (AV), should be tested thoroughly ahead of deployment.  
ADS must satisfy a complex set of rules to ensure road safety, e.g., the existing traffic laws and possibly future laws that are dedicated to AVs.  
To comprehensively test an ADS, we would like to systematically discover diverse scenarios in which certain traffic law is violated. The challenge is that (1) there are many traffic laws (e.g., 13 testable articles in Chinese traffic laws and 16 testable articles in Singapore traffic laws, with 81 and 43 violation situations respectively); and (2) many of traffic laws are only relevant in complicated specific scenarios.  

Existing approaches to testing ADS either focus on simple oracles such as no-collision or have limited capacity in generating diverse law-violating scenarios.  
In this work, we propose ABLE, a new ADS testing method inspired by the success of GFlowNet, which Aims to Break many Laws Efficiently by generating diverse scenarios.  
Different from vanilla GFlowNet, ABLE drives the testing process with dynamically updated testing objectives (based on a robustness semantics of signal temporal logic) as well as active learning, so as to effectively explore the vast search space.  
We evaluate ABLE based on Apollo and LGSVL, and the results show that ABLE outperforms the state-of-the-art by violating 17% and 25% more laws when testing Apollo 6.0 and Apollo 7.0, most of which are hard-to-violate laws, respectively.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {942–953},
numpages = {12},
keywords = {Automated Driving System, Baidu Apollo, Generative Flow Network, Testing Scenario Generation, Traffic Laws},
location = {<conf-loc>, <city>Seattle</city>, <state>WA</state>, <country>USA</country>, </conf-loc>},
series = {ISSTA 2023}
}

@inproceedings{10.1145/3387903.3389306,
author = {Yalla, Muralidhar and Sunil, Asha},
title = {AI-Driven Conversational Bot Test Automation Using Industry Specific Data Cartridges},
year = {2020},
isbn = {9781450379571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387903.3389306},
doi = {10.1145/3387903.3389306},
abstract = {The paper describes an in-house accelerator to generate alternate synonymous sentences and similar intent from sample utterances, the generated data can be applied as test input for conversational AI bots (either text or voice-based). Its NLP-driven sentence generator exposes a RESTful service, which can be consumed by automated testing tools/frameworks such as Katalon, Selenium, and so on. The paper presents building an accelerator to quickly teach and test adaptive conversational AI bots. The approach helps to analyze user inputs and extract intent, the bot developer should ensure a variety of possible utterances are coded. In the traditional manual approach, it is difficult to conceive every possible user utterance before deploying the bot and hence the bot has an early failure rate. This may diminish the usefulness of the bot and the users may stop using the same. Here we propose an AI-driven bot test automation approach using a patent-pending in-house accelerator referenced as LemmaCartridge (LC) in this paper. Testing tools or frameworks can consume LC's data cartridge API for testing the bot AUT and analyze the responses using automated tools/frameworks like Katalon, Selenium and so on until the bot demonstrates desired outcomes under the supervised train, test and adaptive repeatable testing methods yielding quality@speed for the single major goal of testing conversational AI bots. An example of a program used in experiment is described and the results obtained, especially train and test state machines, industry-specific data cartridges that enable to unearth errors in the AI bot under test, are presented.},
booktitle = {Proceedings of the IEEE/ACM 1st International Conference on Automation of Software Test},
pages = {105–107},
numpages = {3},
keywords = {Conversational AI Bot, NLP Automation, Test Data Generation},
location = {Seoul, Republic of Korea},
series = {AST '20}
}

@article{10.1145/3633455,
author = {Berente, Nicholas and Kormylo, Cameron and Rosenkranz, Christoph},
title = {Test-Driven Ethics for Machine Learning},
year = {2024},
issue_date = {May 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {67},
number = {5},
issn = {0001-0782},
url = {https://doi.org/10.1145/3633455},
doi = {10.1145/3633455},
abstract = {Encouraging organizations to adapt a test-driven ethical development approach.},
journal = {Commun. ACM},
month = {may},
pages = {45–47},
numpages = {3}
}

@inproceedings{10.1145/3617553.3617889,
author = {Ren, Wei},
title = {Gamification in Test-Driven Development Practice},
year = {2023},
isbn = {9798400703737},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3617553.3617889},
doi = {10.1145/3617553.3617889},
abstract = {The challenge of effectively developing and sustaining high-performance professional development practices in software engineering education is one that must be addressed. Test-driven development (TDD), an example of a key professional practical activity, is strongly linked to these high-performance practices. To examine the effects of gamification - the use of game design elements in a non-game context - on motivating students to develop and sustain TDD practice, an experiment was conducted and utilized ordinary least squares (OLS) regression to analyze the data. This experiment showed that gamification motivates students to do high-performing TDD practice. More specifically, gamification changes the individual's TDD behavior, increases engagement in the development activity, and the effect continues for a longer period even after gamification has ceased. Furthermore, a positive association between gamification and the maintainability of the team codebase was supported by the data.},
booktitle = {Proceedings of the 2nd International Workshop on Gamification in Software Development, Verification, and Validation},
pages = {38–46},
numpages = {9},
keywords = {Engagement, Gamification, Maintainability, Software Engineering, Test-driven Development},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {Gamify 2023}
}

@inproceedings{10.1145/3486629.3490692,
author = {Aziz, H M Abdul and Islam, A M Hasibul},
title = {A data-driven framework to identify human-critical autonomous vehicle testing and deployment zones},
year = {2021},
isbn = {9781450391177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3486629.3490692},
doi = {10.1145/3486629.3490692},
abstract = {We proposed a data-driven framework that leverages machine learning and econometric modeling techniques to investigate autonomous vehicle (AV) involved crashes and maps human-critical safety factors to operational design domains. The goal is to assist the infrastructure owner-operators in identifying human-critical AV-testing-and-deployment zones based on data-driven insights from both AV-testing data (e.g., California Department of Motor Vehicle AV crash reports) and historical crash data involving only human drivers. First, we analyzed AV crash data collected from the CA DMV website for May 2018 to December 2020 using ML-based and econometric models incorporating attributes such as weather, lighting condition, road surface condition, vehicle miles traveled, and collision type. Later we use the findings to demonstrate the framework's applicability for New York City crash data at the Zip Code level (2012--2021).},
booktitle = {Proceedings of the 14th ACM SIGSPATIAL International Workshop on Computational Transportation Science},
articleno = {5},
numpages = {8},
keywords = {automated vehicles, human factors, operational design domain, traffic safety},
location = {Beijing, China},
series = {IWCTS '21}
}

@inproceedings{10.1145/3632366.3632383,
author = {Stang, Marco and Sommer, Martin and Kraus, David and Sax, Eric},
title = {Improving the Validation of Automotive Self-Learning Systems through the Synergy of Scenario-Based Testing and Metamorphic Relations},
year = {2024},
isbn = {9798400704734},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3632366.3632383},
doi = {10.1145/3632366.3632383},
abstract = {Numerous applications in our everyday life use artificial intelligence (AI) methods for speech and image recognition, as well as the recognition of human behavior. Especially the latter application represents an interesting research field for self-learning systems based on AI methods in the automotive domain. Human driving behavior is determined by routines that an AI system can learn, thereby predicting future actions. However, the methods and tools for validating these systems are insufficient and need to be adapted to the new types of self-learning algorithms. Our framework combines scenario-based testing and metamorphic testing to address the challenges of ensuring correctness and reliability in dynamic and probabilistic SLS. A proof of concept is performed using the example of a self-learning comfort function in a vehicle. The correct functionality is shown by comparing the generated test cases. The concept addresses the main challenges in testing self-learning systems, in particular, the generation of test inputs and the creation of a test oracle.},
booktitle = {Proceedings of the IEEE/ACM 10th International Conference on Big Data Computing, Applications and Technologies},
articleno = {02},
numpages = {7},
keywords = {self-learning systems, scenario-based testing, metamorphic relations, test input generation, test oracle},
location = {<conf-loc>, <city>Taormina (Messina)</city>, <country>Italy</country>, </conf-loc>},
series = {BDCAT '23}
}

@inproceedings{10.1145/3382494.3410687,
author = {Ghafari, Mohammad and Gross, Timm and Fucci, Davide and Felderer, Michael},
title = {Why Research on Test-Driven Development is Inconclusive?},
year = {2020},
isbn = {9781450375801},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382494.3410687},
doi = {10.1145/3382494.3410687},
abstract = {[Background] Recent investigations into the effects of Test-Driven Development (TDD) have been contradictory and inconclusive. This hinders development teams to use research results as the basis for deciding whether and how to apply TDD. [Aim] To support researchers when designing a new study and to increase the applicability of TDD research in the decision-making process in industrial context, we aim at identifying the reasons behind the inconclusive research results in TDD. [Method] We studied the state of the art in TDD research published in top venues in the past decade, and analyzed the way these studies were set up. [Results] We identified five categories of factors that directly impact the outcome of studies on TDD. [Conclusions] This work can help researchers to conduct more reliable studies, and inform practitioners of risks they need to consider when consulting research on TDD.},
booktitle = {Proceedings of the 14th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)},
articleno = {25},
numpages = {10},
keywords = {TDD, Test-Driven Development, industry-academia collaboration, literature review, empirical software engineering, test-first, threats to validity},
location = {Bari, Italy},
series = {ESEM '20}
}

@inproceedings{10.1145/3377325.3377506,
author = {Kundinger, Thomas and Riener, Andreas and Sofra, Nikoletta and Weigl, Klemens},
title = {Driver drowsiness in automated and manual driving: insights from a test track study},
year = {2020},
isbn = {9781450371186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377325.3377506},
doi = {10.1145/3377325.3377506},
abstract = {Driver drowsiness is a major cause of traffic accidents. Automated driving might counteract this problem, but in the lower automation levels, the driver is still responsible as a fallback. The impact of driver drowsiness on automated driving under realistic conditions is, however, currently unknown. This work contributes to risk and hazard assessment with a field study comparing manual to level-2 automated driving. The experiment was conducted on a test track using an instrumented vehicle. Results (N=30) show that in automated driving, driver drowsiness (self-rated with the Karolinska Sleepiness Scale) is significantly higher as compared to manual driving. Age group (20--25, 65--70 years) and driving time have a strong impact on the self-ratings. Additionally, to subjective measures, a correlation was also identified between drowsiness and heart rate data. The gained knowledge can be useful for the development of drowsiness detection systems and dynamic configuration of driver-vehicle interfaces based on user state.},
booktitle = {Proceedings of the 25th International Conference on Intelligent User Interfaces},
pages = {369–379},
numpages = {11},
keywords = {automated driving, driver drowsiness detection, driver state, field study, subjective methods, wearables},
location = {Cagliari, Italy},
series = {IUI '20}
}

@inproceedings{10.1145/3579375.3579414,
author = {Chen, Qifan and Lu, Yang and Tam, Charmaine S. and Poon, Simon K.},
title = {A Data-Driven Framework to Test Validity of the Discovered Clinical Process Based on Selected Patient Outcomes},
year = {2023},
isbn = {9798400700057},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579375.3579414},
doi = {10.1145/3579375.3579414},
abstract = {A clinical process contains a treatment pathway for a well-defined patient group that aims to improve patient outcomes. Electronic Health Records (EHRs) have become popular with the rapid development of healthcare information systems. Process mining utilises event logs extracted from EHRs and aims to discover systematic clinical processes automatically. Hence, the validity of the discovered clinical process becomes a critical issue for stakeholders. Existing methods mainly focus on evaluating the performance of the discovery algorithms (the conformance between the discovered process and the event log) while ignoring the validity of the discovered clinical process. Therefore, we propose a data-driven framework that can test the validity of the process by correlating selected patient outcomes with the discovered clinical process. A valid and effective clinical process should positively impact patient outcomes and can potentially assist in developing evidence-based guidelines for future treatments. The data-driven framework adopts the triangulation method combined with the statistical test model consisting of propensity score matching and difference-in-difference regression to overcome the limitations of existing methods. A use case scenario is presented to discover the clinical process for patients with suspected acute coronary syndrome.},
booktitle = {Proceedings of the 2023 Australasian Computer Science Week},
pages = {248–251},
numpages = {4},
keywords = {Healthcare process, Process mining, Process validation},
location = {<conf-loc>, <city>Melbourne</city>, <state>VIC</state>, <country>Australia</country>, </conf-loc>},
series = {ACSW '23}
}

@inproceedings{10.1109/ICSE-SEIP58684.2023.00016,
author = {Tuli, Shreshth and Bojarczuk, Kinga and Gucevska, Natalija and Harman, Mark and Wang, Xiao-Yu and Wright, Graham},
title = {Simulation-Driven Automated End-to-End Test and Oracle Inference},
year = {2023},
isbn = {9798350300376},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP58684.2023.00016},
doi = {10.1109/ICSE-SEIP58684.2023.00016},
abstract = {This is the first work to report on inferential testing at scale in industry. Specifically, it reports the experience of automated testing of integrity systems at Meta. We built an internal tool called ALPACAS for automated inference of end-to-end integrity tests. Integrity tests are designed to keep users safe online by checking that interventions take place when harmful behaviour occurs on a platform. ALPACAS infers not only the test input, but also the oracle, by observing production interventions to prevent harmful behaviour. This approach allows Meta to automate the process of generating integrity tests for its platforms, such as Facebook and Instagram, which consist of hundreds of millions of lines of production code. We outline the design and deployment of ALPACAS, and report results for its coverage, number of tests produced at each stage of the test inference process, and their pass rates. Specifically, we demonstrate that using ALPACAS significantly improves coverage from a manual test design for the particular aspect of integrity end-to-end testing it was applied to. Further, from a pool of 3 million data points, ALPACAS automatically yields 39 production-ready end-to-end integrity tests. We also report that the ALPACAS-inferred test suite enjoys exceptionally low flakiness for end-to-end testing with its average in-production pass rate of 99.84%.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Software Engineering in Practice},
pages = {122–133},
numpages = {12},
keywords = {automated test design, oracle problem, automated oracle inference, test automation, safety testing, integrity testing},
location = {Melbourne, Australia},
series = {ICSE-SEIP '23}
}

@inproceedings{10.5555/3545946.3599165,
author = {Amaral, Cleber Jorge and H\"{u}bner, Jomi Fred and Kampik, Timotheus},
title = {TDD for AOP: Test-Driven Development for Agent-Oriented Programming},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {This demonstration paper introduces native test-driven development capabilities that have been implemented in an agent-oriented programming language, in particular as extensions of AgentSpeak. We showcase how these capabilities can facilitate the testing and continuous integration of agents in JaCaMo multi-agent systems.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {3038–3040},
numpages = {3},
location = {<conf-loc>, <city>London</city>, <country>United Kingdom</country>, </conf-loc>},
series = {AAMAS '23}
}

@article{10.1145/3579642,
author = {Tang, Shuncheng and Zhang, Zhenya and Zhang, Yi and Zhou, Jixiang and Guo, Yan and Liu, Shuang and Guo, Shengjian and Li, Yan-Fu and Ma, Lei and Xue, Yinxing and Liu, Yang},
title = {A Survey on Automated Driving System Testing: Landscapes and Trends},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {5},
issn = {1049-331X},
url = {https://doi.org/10.1145/3579642},
doi = {10.1145/3579642},
abstract = {Automated Driving Systems (ADS) have made great achievements in recent years thanks to the efforts from both academia and industry. A typical ADS is composed of multiple modules, including sensing, perception, planning, and control, which brings together the latest advances in different domains. Despite these achievements, safety assurance of ADS is of great significance, since unsafe behavior of ADS can bring catastrophic consequences. Testing has been recognized as an important system validation approach that aims to expose unsafe system behavior; however, in the context of ADS, it is extremely challenging to devise effective testing techniques, due to the high complexity and multidisciplinarity of the systems. There has been great much literature that focuses on the testing of ADS, and a number of surveys have also emerged to summarize the technical advances. Most of the surveys focus on the system-level testing performed within software simulators, and they thereby ignore the distinct features of different modules. In this article, we provide a comprehensive survey on the existing ADS testing literature, which takes into account both module-level and system-level testing. Specifically, we make the following contributions: (1) We survey the module-level testing techniques for ADS and highlight the technical differences affected by the features of different modules; (2) we also survey the system-level testing techniques, with focuses on the empirical studies that summarize the issues occurring in system development or deployment, the problems due to the collaborations between different modules, and the gap between ADS testing in simulators and the real world; and (3) we identify the challenges and opportunities in ADS testing, which pave the path to the future research in this field.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {jul},
articleno = {124},
numpages = {62},
keywords = {ADS testing, module-level testing, system-level testing, system security}
}

@article{10.5555/3512469.3512472,
author = {Sprint, Gina},
title = {Teaching test-driven development of algorithms behind data science library APIs},
year = {2021},
issue_date = {October 2021},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {37},
number = {1},
issn = {1937-4771},
abstract = {Members of a growing community of data science and machine learning experts are advocating for the increased use of explainable and interpretable algorithms. To help students understand these algorithms, this paper presents a programming-intensive course, CPSC 322 Data Science Algorithms, that utilizes a modern data science technology stack to introduce students to fundamental and explainable data science algorithms. In this course, students utilize test-driven development (TDD) to implement data science algorithms "from scratch," while adhering to the same application programming interface (API) as the industry-standard library. With this approach to teaching data science, students learn popular library APIs by building their own mini version of the libraries. They also learn the "what" and the "why" behind the libraries instead of mostly using them as black boxes. Along the way, students in CPSC 322 also gain experience with software engineering tools in an industry-standard data science tech stack, including Docker, Git/Github, automated testing, and machine learning model deployment with Flask and Heroku. Over the duration of the course, students' experience with these tools, API programming, and TDD increase significantly.},
journal = {J. Comput. Sci. Coll.},
month = {oct},
pages = {18–27},
numpages = {10}
}

@inproceedings{10.1145/3404835.3463243,
author = {Afzali, Jafar and Drzewiecki, Aleksander Mark and Balog, Krisztian},
title = {POINTREC: A Test Collection for Narrative-driven Point of Interest Recommendation},
year = {2021},
isbn = {9781450380379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404835.3463243},
doi = {10.1145/3404835.3463243},
abstract = {This paper presents a test collection for contextual point of interest (POI) recommendation in a narrative-driven scenario. There, user history is not available, instead, user requests are described in natural language. The requests in our collection are manually collected from social sharing websites, and are annotated with various types of metadata, including location, categories, constraints, and example POIs. These requests are to be resolved from a dataset of POIs, which are collected from a popular online directory, and are further linked to a geographical knowledge base and enriched with relevant web snippets. Graded relevance assessments are collected using crowdsourcing, by pooling both manual and automatic recommendations, where the latter serve as baselines for future performance comparison. This resource supports the development of novel approaches for end-to-end POI recommendation as well as for specific semantic annotation tasks on natural language requests.},
booktitle = {Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {2478–2484},
numpages = {7},
keywords = {POI recommendation, cold start recommendation, contextual suggestions, narrative-driven recommendation, point of interest},
location = {<conf-loc>, <city>Virtual Event</city>, <country>Canada</country>, </conf-loc>},
series = {SIGIR '21}
}

@inproceedings{10.1109/ICSE-Companion58688.2023.00085,
author = {Deng, Wenjing},
title = {AIGROW: A Feedback-Driven Test Generation Framework for Hardware Model Checkers},
year = {2023},
isbn = {9798350322637},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion58688.2023.00085},
doi = {10.1109/ICSE-Companion58688.2023.00085},
abstract = {This research abstract introduces an effective and efficient approach to automatically generate high-quality hardware model checker benchmarks. The key contribution of this work is to model the input format of hardware model checkers using a tree-based structure named ARTree and build an effective feedback-driven test generation framework based on ARTree named AIGROW. The evaluation shows that AIGROW generates very small but high-quality benchmarks for coverage-oriented and performance-oriented testing and outperforms the existing generation-based testing tools.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Companion Proceedings},
pages = {314–316},
numpages = {3},
keywords = {test generation, hardware model checker},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3377049.3377076,
author = {Intisar, Arik and Islam, Md Khaled Ben and Rahman, Julia},
title = {A Deep Convolutional Neural Network Based Small Scale Test-bed for Autonomous Car},
year = {2020},
isbn = {9781450377782},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377049.3377076},
doi = {10.1145/3377049.3377076},
abstract = {In deep end-to-end learning based autonomous car design, inferencing the signal by trained model is one of the critical issues, particularly, in case of embedded component. Researchers from both academia and industry have been putting their enormous efforts in making this critical autonomous driving more reliable and safer. As research on the real car is costly and poses safety issue, we have developed a small scale, low-cost, deep convolutional neural network powered self-driving car model. Its learning model adopted from NVIDIA's DAVE-2 which is a real autonomous car and University of Kansas' small scale DeepPicar. Similar to DAVE-2, its neural architecture uses 5 convolution layer and 3 fully connected layers with 250,000 parameters. We have considered Raspberry Pi 3B+ as the processing platform with Quad-core 1.4 GHz CPU based on A53 architecture which is capable to support CNN learning model.},
booktitle = {Proceedings of the International Conference on Computing Advancements},
articleno = {20},
numpages = {5},
keywords = {Autonomous Car, CNN, End-to-End Learning, Low-cost, Raspberry Pie},
location = {<conf-loc>, <city>Dhaka</city>, <country>Bangladesh</country>, </conf-loc>},
series = {ICCA 2020}
}

@inproceedings{10.1145/3544902.3546233,
author = {Romano, Simone and Zampetti, Fiorella and Baldassarre, Maria Teresa and Di Penta, Massimiliano and Scanniello, Giuseppe},
title = {Do Static Analysis Tools Affect Software Quality when Using Test-driven Development?},
year = {2022},
isbn = {9781450394277},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544902.3546233},
doi = {10.1145/3544902.3546233},
abstract = {Background. Test-Driven Development (TDD) is an agile software development practice, which encourages developers to write “quick-and-dirty” production code to make tests pass, and then apply refactoring to “clean” written code. However, previous studies have found that refactoring is not applied as often as the TDD process requires, potentially affecting software quality. Aims. We investigated the benefits of leveraging a Static Analysis Tool (SAT)—plugged-in the Integrated Development Environment (IDE)—on software quality, when applying TDD. Method. We conducted two controlled experiments, in which the participants—92, in total—performed an implementation task by applying TDD with or without a SAT highlighting the presence of code smells in their source code. We then analyzed the effect of the used SAT on software quality. Results. We found that, overall, the use of a SAT helped the participants to significantly improve software quality, yet the participants perceived TDD more difficult to be performed. Conclusions. The obtained results may impact: (i)&nbsp;practitioners, helping them improve their TDD practice through the adoption of proper settings and tools; (ii)&nbsp;educators, in better introducing TDD within their courses; and (iii)&nbsp;researchers, interested in developing better tool support for developers, or further studying&nbsp;TDD.},
booktitle = {Proceedings of the 16th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {80–91},
numpages = {12},
keywords = {Refactoring, Static Analysis Tool, Test-driven Development},
location = {Helsinki, Finland},
series = {ESEM '22}
}

@inproceedings{10.1145/3411016.3411158,
author = {Xiao, Litian and Xiao, Nan and Li, Mengyuan and Xie, Shanshan},
title = {Relativity-Driven Optimization for Test Schedule of Spaceflight Products at Launch Site},
year = {2020},
isbn = {9781450375498},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411016.3411158},
doi = {10.1145/3411016.3411158},
abstract = {The test preparation of flight products determines the length of the entire cycle from flight products entering the launch site to launch. The improvement of test efficiency is beneficial to the launch efficiency of the launch site. According to the test scheduling plan of flight products at the launch site, we propose test scheduling model based on related data-driven optimization and superimpose test elements on the test network schedule diagram. By means of the correlation between test item input, excitation and output response, the optimization strategy of test process was proposed to decompose and shorten the critical path on test network schedule. The strategy shortens the test cycle of flight products. Through practice, integration testing and good results have been obtained.},
booktitle = {Proceedings of the 2nd International Conference on Industrial Control Network And System Engineering Research},
pages = {49–56},
numpages = {8},
keywords = {Network Scheduling, Optimization strategy, Relativity-driven optimization, Spaceflight products test, Test schedule},
location = {Kuala Lumpur, Malaysia},
series = {ICNSER2020}
}

@inproceedings{10.1109/ICSE-NIER58687.2023.00025,
author = {Calais, Pedro and Franzini, Lissa},
title = {Test-Driven Development Benefits beyond Design Quality: Flow State and Developer Experience},
year = {2023},
isbn = {9798350300390},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-NIER58687.2023.00025},
doi = {10.1109/ICSE-NIER58687.2023.00025},
abstract = {Test-driven development (TDD) is a coding technique that combines design and testing in an iterative and incremental fashion. It prescribes that tests written before the production code help the developer to find good interfaces and to evolve the design safely and incrementally. Improvements on the design of code produced by the test-driven development approach have been extensively evaluated in the literature; in this research, we focus on seeking explanations on the benefits of TDD in another dimension which we believe has been undervalued - developer experience. We identified that there is a natural connection between the TDD approach and flow state, a well-known mental state characterized by total immersion, focus, and involvement in a task that promotes increased enjoyment and productivity. We present evidence that the continuous stream of mini-scope, shortlived, red-green-refactor cycles of TDD frame the development task as a structure that creates the pre-conditions reported by neuroscience research to produce flow state, namely (1) clear goals, (2) unambiguous feedback, (3) challenge-skill balance and (4) sense of control. Our work contributes to increase the understanding on the reasons why adopting practices such as TDD can benefit the software development process as a whole and can support its adoption in software development projects.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {106–111},
numpages = {6},
keywords = {TDD, developer experience, flow state},
location = {Melbourne, Australia},
series = {ICSE-NIER '23}
}

@article{10.1145/3555613,
author = {Malkin, Nathan and Wagner, David and Egelman, Serge},
title = {Can Humans Detect Malicious Always-Listening Assistants? A Framework for Crowdsourcing Test Drives},
year = {2022},
issue_date = {November 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {CSCW2},
url = {https://doi.org/10.1145/3555613},
doi = {10.1145/3555613},
abstract = {As intelligent voice assistants become more widespread and the scope of their listening increases, they become attractive targets for attackers. In the future, a malicious actor could train voice assistants to listen to audio outside their purview, creating a threat to users' privacy and security. How can this misbehavior be detected? Due to the ambiguities of natural language, people may need to work in conjunction with algorithms to determine whether a given conversation should be heard. To investigate how accurately humans can perform this task, we developed a framework for people to conduct "Test Drives" of always-listening services: after submitting sample conversations, users receive instant feedback about whether these would have been captured. Leveraging a Wizard of Oz interface, we conducted a study with 200 participants to determine whether they could detect one of four types of attacks on three different services. We studied the behavior of individuals, as well as groups working collaboratively, and investigated the effects of task framing on performance. We found that individuals were able to successfully detect malicious apps at varying rates (7.5% to 75%), depending on the type of malicious attack, and that groups were highly successful when considered collectively. Our results suggest that the Test Drive framework can be an effective tool for studying user behaviors and concerns, as well as a potentially welcome addition to voice assistant app stores, where it could decrease privacy concerns surrounding always-listening services.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {nov},
articleno = {500},
numpages = {28},
keywords = {crowdsourcing, intelligent assistants, passive listening, usable security and privacy}
}

@article{10.1145/3450356,
author = {Abrecht, Stephanie and Gauerhof, Lydia and Gladisch, Christoph and Groh, Konrad and Heinzemann, Christian and Woehrle, Matthias},
title = {Testing Deep Learning-based Visual Perception for Automated Driving},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
issn = {2378-962X},
url = {https://doi.org/10.1145/3450356},
doi = {10.1145/3450356},
abstract = {Due to the impressive performance of deep neural networks (DNNs) for visual perception, there is an increased demand for their use in automated systems. However, to use deep neural networks in practice, novel approaches are needed, e.g., for testing. In this work, we focus on the question of how to test deep learning-based visual perception functions for automated driving. Classical approaches for testing are not sufficient: A purely statistical approach based on a dataset split is not enough, as testing needs to address various purposes and not only average case performance. Additionally, a complete specification is elusive due to the complexity of the perception task in the open context of automated driving. In this article, we review and discuss existing work on testing DNNs for visual perception with a special focus on automated driving for test input and test oracle generation as well as test adequacy. We conclude that testing of DNNs in this domain requires several diverse test sets. We show how such tests sets can be constructed based on the presented approaches addressing different purposes based on the presented methods and identify open research questions.},
journal = {ACM Trans. Cyber-Phys. Syst.},
month = {sep},
articleno = {37},
numpages = {28},
keywords = {Software testing, deep learning, perception, computer vision, automated driving, autonomous driving}
}

@inproceedings{10.1145/3551349.3559549,
author = {Wang, Xin and Liu, Xiao and Zhou, Pingyi and Liu, Qixia and Liu, Jin and Wu, Hao and Cui, Xiaohui},
title = {Test-Driven Multi-Task Learning with Functionally Equivalent Code Transformation for Neural Code Generation},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3559549},
doi = {10.1145/3551349.3559549},
abstract = {Automated code generation is a longstanding challenge in both communities of software engineering and artificial intelligence. Currently, some works have started to investigate the functional correctness of code generation, where a code snippet is considered correct if it passes a set of test cases. However, most existing works still model code generation as text generation without considering program-specific information, such as functionally equivalent code snippets and test execution feedback. To address the above limitations, this paper proposes a method combining program analysis with deep learning for neural code generation, where functionally equivalent code snippets and test execution feedback will be considered at the training stage. Concretely, we firstly design several code transformation heuristics to produce different variants of the code snippet satisfying the same functionality. In addition, we employ the test execution feedback and design a test-driven discriminative task to train a novel discriminator, aiming to let the model distinguish whether the generated code is correct or not. The preliminary results on a newly published dataset demonstrate the effectiveness of our proposed framework for code generation. Particularly, in terms of the pass@1 metric, we achieve 8.81 and 11.53 gains compared with CodeGPT and CodeT5, respectively.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {188},
numpages = {6},
keywords = {Code Transformation, Execution Feedback, Multi-Task Learning, Neural Code Generation, Program Analysis},
location = {<conf-loc>, <city>Rochester</city>, <state>MI</state>, <country>USA</country>, </conf-loc>},
series = {ASE '22}
}

@inproceedings{10.1109/ICSE43902.2021.00139,
author = {Yu, Shengcheng and Fang, Chunrong and Yun, Yexiao and Feng, Yang},
title = {Layout and Image Recognition Driving Cross-Platform Automated Mobile Testing},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00139},
doi = {10.1109/ICSE43902.2021.00139},
abstract = {The fragmentation problem has extended from Android to different platforms, such as iOS, mobile web, and even mini-programs within some applications (app), like WeChat1. In such a situation, recording and replaying test scripts is one of the most popular automated mobile app testing approaches. However, such approach encounters severe problems when crossing platforms. Different versions of the same app need to be developed to support different platforms relying on different platform supports. Therefore, mobile app developers need to develop and maintain test scripts for multiple platforms aimed at completely the same test requirements, greatly increasing testing costs. However, we discover that developers adopt highly similar user interface layouts for versions of the same app on different platforms. Such a phenomenon inspires us to replay test scripts from the perspective of similar UI layouts.In this paper, we propose an image-driven mobile app testing framework, utilizing Widget Feature Matching and Layout Characterization Matching to analyze app UIs. We use computer vision (CV) technologies to perform UI feature comparison and layout hierarchy extraction on mobile app screenshots to obtain UI structures containing rich contextual information of app widgets, including coordinates, relative relationship, etc. Based on acquired UI structures, we can form a platform-independent test script, and then locate the target widgets under test. Thus, the proposed framework non-intrusively replays test scripts according to a novel platform-independent test script model. We also design and implement a tool named LIRAT to devote the proposed framework into practice, based on which, we conduct an empirical study to evaluate the effectiveness and usability of the proposed testing framework. The results show that the overall replay accuracy reaches around 65.85% on Android (8.74% improvement over state-of-the-art approaches) and 35.26% on iOS (35% improvement over state-of-the-art approaches).},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {1561–1571},
numpages = {11},
keywords = {Cross-Platform Testing, Image Analysis, Mobile Testing, Record and Replay},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{10.1145/3609437.3609449,
author = {Liu, Lei and Wang, Sinan and Liu, Yepang and Deng, Jinliang and Liu, Sicen},
title = {Drift: Fine-Grained Prediction of the Co-Evolution of Production and Test Code via Machine Learning},
year = {2023},
isbn = {9798400708947},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3609437.3609449},
doi = {10.1145/3609437.3609449},
abstract = {As production code evolves, test code can quickly become outdated. When test code is outdated, it may fail to capture errors in the programs under test and can lead to serious software bugs that result in significant losses for both developers and users. To ensure high software quality, it is crucial to promptly update the test code after making changes to the production code. This practice ensures that the test code and production code evolve together, reducing the likelihood of errors and ensuring the software remains reliable. However, maintaining test code can be challenging and time-consuming. To automate the identification of outdated test code, recent research has proposed Sitar, a machine learning-based method. Despite Sitar’s usefulness, it has major limitations, including its coarse prediction granularity (at class level), reliance on naming conventions to discover test code, and dependence on manually summarized features to construct machine learning models. In this paper, we address the limitations of Sitar and propose a new machine learning-based approach Drift. Drift&nbsp;predicts outdated test cases at the method level. It leverages method-calling relationships to accurately infer the links between production and test code, and automatically learns features via code analysis. We evaluate Drift&nbsp;using 40 open-source Java projects in both within-project and cross-project scenarios, and find that Drift&nbsp;can achieve satisfactory prediction performances in both scenarios. We also compare Drift&nbsp;with existing methods for outdated test code prediction and find that Drift&nbsp;can significantly outperform them. For example, compared with Sitar, the accuracy of Drift&nbsp;is increased by about 8.5%, the F1-score is increased by about 8.3%, and more importantly, the number of test cases that developers need to check is reduced by about 75%. Therefore, our method, Drift, can predict outdated test cases more accurately at a fine-grained level, and thus better facilitate the co-evolution of production and test code.},
booktitle = {Proceedings of the 14th Asia-Pacific Symposium on Internetware},
pages = {227–237},
numpages = {11},
keywords = {Machine Learning, Outdated Test Code, Software Evolution},
location = {<conf-loc>, <city>Hangzhou</city>, <country>China</country>, </conf-loc>},
series = {Internetware '23}
}

@inproceedings{10.1145/3473682.3480269,
author = {Henze, Franziska and Stasinski, Natalie Magdalena and Fassbender, Dennis and Stiller, Christoph},
title = {Developers’ Information Needs during Test Drives with Automated Vehicles in Real Traffic: A Focus Group Study},
year = {2021},
isbn = {9781450386418},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3473682.3480269},
doi = {10.1145/3473682.3480269},
abstract = {So far, user interfaces are mainly developed for customers using highly automated driving functions, but little is known about the requirements on human machine interfaces for function developers. However, good development interfaces are important for efficient validating in real traffic test drives. Therefore, we present a focus group study with software developers of such functions to characterize their needs. Firstly, we identify typical situations where the automated vehicle executes an action other than the one expected, and, secondly, we gather developers’ requests for information to increase the traceability of driving decisions in general. We found that developers perceive actions involving lateral motion as being more non-transparent than with longitudinal motion and therefore identify a need for a more detailed explanation of these. Additionally, explanations should include more environmental features to help understand the reasons behind an action, and at the same time be tailored to the developers’ information interests.},
booktitle = {13th International Conference on Automotive User Interfaces and Interactive Vehicular Applications},
pages = {81–85},
numpages = {5},
keywords = {automated driving, decision-making, development interface, explanation, focus group study, information presentation, transparency},
location = {Leeds, United Kingdom},
series = {AutomotiveUI '21 Adjunct}
}

@inproceedings{10.1145/3365438.3410991,
author = {Anjorin, Anthony and Weidmann, Nils and Oppermann, Robin and Fritsche, Lars and Sch\"{u}rr, Andy},
title = {Automating test schedule generation with domain-specific languages: a configurable, model-driven approach},
year = {2020},
isbn = {9781450370196},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3365438.3410991},
doi = {10.1145/3365438.3410991},
abstract = {Solving scheduling problems is important for a wide range of application domains including home care in the health care domain, allocation engineering in the automotive domain, and virtual network embedding in the network virtualisation domain. Standard solution approaches assume that an initially given problem definition (e.g. a set of constraints and an objective function) can be fixed, and does not have to be constantly changed and validated by domain experts. In this paper, we investigate an application where this is not the case: at dSPACE GmbH, a developer of software and hardware for mechatronic control systems, recurring manual tests must be executed in every development and release cycle. To allocate human resources (developers and testers) to perform these tests, a test schedule must be created and maintained during the testing process. Prior to our work, test scheduling at dSPACE was performed manually by a test manager, requiring more than one working day to create the initial schedule, and several hours of tedious, error-prone work every week to maintain the schedule. The novel challenge here is that an acceptable automation must be highly configurable by the test manager (the domain expert), who should be able to easily adapt and validate the problem definition on a regular basis. We demonstrate that techniques and results from consistency maintenance via triple graph grammars, and constraint solving via linear programming can be synergetically combined to yield a highly configurable and fully automated approach to test schedule generation. We evaluate our solution at dSPACE and show that it not only reduces the effort required to create and maintain schedules of acceptable quality, but that it can also be understood, configured, and validated by the test manager.},
booktitle = {Proceedings of the 23rd ACM/IEEE International Conference on Model Driven Engineering Languages and Systems},
pages = {320–331},
numpages = {12},
keywords = {allocation engineering, consistency management, domain-specific language, linear programming, triple graph grammar},
location = {Virtual Event, Canada},
series = {MODELS '20}
}

@inproceedings{10.5555/3545946.3598761,
author = {Rodriguez, Sebastian and Thangarajah, John and Winikoff, Michael},
title = {A Behaviour-Driven Approach for Testing Requirements via User and System Stories in Agent Systems},
year = {2023},
isbn = {9781450394321},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Testing is a critical part of the software development cycle. This is even more important for autonomous systems, which can be challenging to test. In mainstream software engineering, Behaviour-Driven Development (BDD) is an Agile software development practice that is well accepted and widely used. It involves defining test cases for the expected system behaviour prior to developing the associated functionality. In this work, we present a BDD approach to testing the behavioural requirements of an agent system specified via User and System Stories (USS). USS is also based on established Agile processes and is shown to be intuitive and readily mapped to agent concepts. More specifically we extend USS so that they can be used for testing, and develop a behaviour-driven testing framework based on USS. We show how test cases can be developed, and how to evaluate the test cases by using a state-of-the-art mutation testing system, PITest, which we have integrated into our test framework. A key feature of our work is that we leverage a range of state-of-the-art development tools, inheriting the rich set of features they provide.},
booktitle = {Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems},
pages = {1182–1190},
numpages = {9},
keywords = {AOSE, behaviour-driven development, requirements testing, system stories, test-driven development testing, user stories},
location = {<conf-loc>, <city>London</city>, <country>United Kingdom</country>, </conf-loc>},
series = {AAMAS '23}
}

@inproceedings{10.1145/3460319.3464829,
author = {Liu, Zixi and Feng, Yang and Chen, Zhenyu},
title = {DialTest: automated testing for recurrent-neural-network-driven dialogue systems},
year = {2021},
isbn = {9781450384599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460319.3464829},
doi = {10.1145/3460319.3464829},
abstract = {With the tremendous advancement of recurrent neural networks(RNN), dialogue systems have achieved significant development. Many RNN-driven dialogue systems, such as Siri, Google Home, and Alexa, have been deployed to assist various tasks. However, accompanying this outstanding performance, RNN-driven dialogue systems, which are essentially a kind of software, could also produce erroneous behaviors and result in massive losses. Meanwhile, the complexity and intractability of RNN models that power the dialogue systems make their testing challenging. In this paper, we design and implement DialTest, the first RNN-driven dialogue system testing tool. DialTest employs a series of transformation operators to make realistic changes on seed data while preserving their oracle information properly. To improve the efficiency of detecting faults, DialTest further adopts Gini impurity to guide the test generation process. We conduct extensive experiments to validate DialTest. We first experiment it on two fundamental tasks, i.e., intent detection and slot filling, of natural language understanding. The experiment results show that DialTest can effectively detect hundreds of erroneous behaviors for different RNN-driven natural language understanding (NLU) modules of dialogue systems and improve their accuracy via retraining with the generated data. Further, we conduct a case study on an industrial dialogue system to investigate the performance of DialTest under the real usage scenario. The study shows DialTest can detect errors and improve the robustness of RNN-driven dialogue systems effectively.},
booktitle = {Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {115–126},
numpages = {12},
keywords = {automated testing, deep learning testing, dialog system testing},
location = {Virtual, Denmark},
series = {ISSTA 2021}
}

@inproceedings{10.1145/3533767.3534391,
author = {Ji, Pin and Feng, Yang and Liu, Jia and Zhao, Zhihong and Chen, Zhenyu},
title = {ASRTest: automated testing for deep-neural-network-driven speech recognition systems},
year = {2022},
isbn = {9781450393799},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3533767.3534391},
doi = {10.1145/3533767.3534391},
abstract = {With the rapid development of deep neural networks and end-to-end learning techniques, automatic speech recognition (ASR) systems have been deployed into our daily and assist in various tasks. However, despite their tremendous progress, ASR systems could also suffer from software defects and exhibit incorrect behaviors. While the nature of DNN makes conventional software testing techniques inapplicable for ASR systems, lacking diverse tests and oracle information further hinders their testing. In this paper, we propose and implement a testing approach, namely ASR, specifically for the DNN-driven ASR systems. ASRTest is built upon the theory of metamorphic testing. We first design the metamorphic relation for ASR systems and then implement three families of transformation operators that can simulate practical application scenarios to generate speeches. Furthermore, we adopt Gini impurity to guide the generation process and improve the testing efficiency. To validate the effectiveness of ASRTest, we apply ASRTest to four ASR models with four widely-used datasets. The results show that ASRTest can detect erroneous behaviors under different realistic application conditions efficiently and improve 19.1% recognition performance on average via retraining with the generated data. Also, we conduct a case study on an industrial ASR system to investigate the performance of ASRTest under the real usage scenario. The study shows that ASRTest can detect errors and improve the performance of DNN-driven ASR systems effectively.},
booktitle = {Proceedings of the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {189–201},
numpages = {13},
keywords = {Automated Testing, Automatic Speech Recognition, Deep Neural Networks, Metamorphic Testing},
location = {<conf-loc>, <city>Virtual</city>, <country>South Korea</country>, </conf-loc>},
series = {ISSTA 2022}
}

@inproceedings{10.1145/3426020.3426114,
author = {Choi, Jongmoo and Lee, Kwanghee},
title = {Evaluation of Drowsy Driver Detection Schemes: Feasibility Test for Processing-in-Vehicle},
year = {2021},
isbn = {9781450389259},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3426020.3426114},
doi = {10.1145/3426020.3426114},
abstract = {These days, automotive vendors are trying to provide a drowsy driver detection capability as a key ingredient of ADAS (Advanced Driver Assistance System). However, materializing this detection capability raises a concern regarding processing-in-vehicle due to the limited computing resources. In this paper, we evaluate several drowsy driver detection schemes including computer vision based and deep learning based scheme on our experimental embedded board. In addition, we devise a new robust detection scheme to enhance detection coverage. Evaluation results demonstrate that schemes considered in this study can identify drowsiness with the response time ranging from 1 to 3 seconds and memory footprint ranging from 107 to 684 MB. Our proposal shows a better ability to recognize drowsiness under poor lighting conditions and using partial images.},
booktitle = {The 9th International Conference on Smart Media and Applications},
pages = {329–332},
numpages = {4},
keywords = {Computer Vision, Deep Learning, Drowsy Driver Detection, Evaluation, Open-face},
location = {Jeju, Republic of Korea},
series = {SMA 2020}
}

@inproceedings{10.1145/3615366.3625077,
author = {Proma, Nawshin Mannan and Alexander, Rob},
title = {Systematic Situation Coverage versus Random Situation Coverage for Safety Testing in an Autonomous Car Simulation},
year = {2023},
isbn = {9798400708442},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3615366.3625077},
doi = {10.1145/3615366.3625077},
abstract = {Autonomous vehicles (AV) have the potential to improve road transport, but faults in the autonomous driving software can result in serious accidents. To assess the safety of AV driving software, we need to consider the wide variety and diversity of situations that it may encounter. Explicit situation coverage has previously been presented, but its usefulness has received a little empirical scrutiny. In this study, we evaluate a situation coverage based safety testing approach by comparing the performance of random and situation coverage-based test generation in terms of its ability to detect seeded faults in our ego AV at a road intersection under diverse environmental conditions. Our results suggest that this implementation of situation coverage, at least, does not provide an advantage over random generation.},
booktitle = {Proceedings of the 12th Latin-American Symposium on Dependable and Secure Computing},
pages = {208–213},
numpages = {6},
keywords = {Autonomous car, Safety, Situation coverage, Testing},
location = {<conf-loc>, <city>La Paz</city>, <country>Bolivia</country>, </conf-loc>},
series = {LADC '23}
}

@inproceedings{10.5555/3408352.3408633,
author = {Floridia, Andrea and Carmona, Tzamn Melendez and Piumatti, Davide and Ruospo, Annachiara and Sanchez, Ernesto and De Luca, Sergio and Martorana, Rosario and Pernice, Mose Alessandro},
title = {Deterministic cache-based execution of on-line self-test routines in multi-core automotive system-on-chips},
year = {2020},
isbn = {9783981926347},
publisher = {EDA Consortium},
address = {San Jose, CA, USA},
abstract = {Traditionally, the usage of caches and deterministic execution of on-line self-test procedures have been considered two mutually exclusive concepts. At the same time, software executed in a multi-core context suffers of a limited timing predictability due to the higher system bus contention. When dealing with self-test procedures, this higher contention might lead to a fluctuating fault coverage or even the failure of some test programs. This paper presents a cache-based strategy for achieving both deterministic behaviour and stable fault coverage from the execution of self-test procedures in multi-core systems. The proposed strategy is applied to two representative modules negatively affected by a multi-core execution: synchronous imprecise interrupts logic and pipeline hazard detection unit. The experiments illustrate that it is possible to achieve a stable execution while also improving the state-of-the-art approaches for the on-line testing of embedded microprocessors. The effectiveness of the methodology was assessed on all the three cores of a multi-core industrial System-on-Chip intended for automotive ASIL D applications.},
booktitle = {Proceedings of the 23rd Conference on Design, Automation and Test in Europe},
pages = {1235–1240},
numpages = {6},
location = {<conf-loc>, <city>Grenoble</city>, <country>France</country>, </conf-loc>},
series = {DATE '20}
}

@inproceedings{10.1145/3377811.3380391,
author = {Gerasimou, Simos and Eniser, Hasan Ferit and Sen, Alper and Cakan, Alper},
title = {Importance-driven deep learning system testing},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380391},
doi = {10.1145/3377811.3380391},
abstract = {Deep Learning (DL) systems are key enablers for engineering intelligent applications due to their ability to solve complex tasks such as image recognition and machine translation. Nevertheless, using DL systems in safety- and security-critical applications requires to provide testing evidence for their dependable operation. Recent research in this direction focuses on adapting testing criteria from traditional software engineering as a means of increasing confidence for their correct behaviour. However, they are inadequate in capturing the intrinsic properties exhibited by these systems. We bridge this gap by introducing DeepImportance, a systematic testing methodology accompanied by an Importance-Driven (IDC) test adequacy criterion for DL systems. Applying IDC enables to establish a layer-wise functional understanding of the importance of DL system components and use this information to assess the semantic diversity of a test set. Our empirical evaluation on several DL systems, across multiple DL datasets and with state-of-the-art adversarial generation techniques demonstrates the usefulness and effectiveness of DeepImportance and its ability to support the engineering of more robust DL systems.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {702–713},
numpages = {12},
keywords = {deep learning systems, safety-critical systems, test adequacy},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1145/3377812.3390793,
author = {Gerasimou, Simos and Eniser, Hasan Ferit and Sen, Alper and Cakan, Alper},
title = {Importance-driven deep learning system testing},
year = {2020},
isbn = {9781450371223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377812.3390793},
doi = {10.1145/3377812.3390793},
abstract = {Deep Learning (DL) systems are key enablers for engineering intelligent applications. Nevertheless, using DL systems in safety- and security-critical applications requires to provide testing evidence for their dependable operation. We introduce DeepImportance, a systematic testing methodology accompanied by an Importance-Driven (IDC) test adequacy criterion for DL systems. Applying IDC enables to establish a layer-wise functional understanding of the importance of DL system components and use this information to assess the semantic diversity of a test set. Our empirical evaluation on several DL systems and across multiple DL datasets demonstrates the usefulness and effectiveness of DeepImportance.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Companion Proceedings},
pages = {322–323},
numpages = {2},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1145/3611643.3613894,
author = {Formica, Federico and Petrunti, Nicholas and Bruck, Lucas and Pantelic, Vera and Lawford, Mark and Menghi, Claudio},
title = {Test Case Generation for Drivability Requirements of an Automotive Cruise Controller: An Experience with an Industrial Simulator},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3613894},
doi = {10.1145/3611643.3613894},
abstract = {Automotive software development requires engineers to test their systems to detect violations of both functional and drivability requirements. Functional requirements define the functionality of the automotive software. Drivability requirements refer to the driver's perception of the interactions with the vehicle; for example, they typically require limiting the acceleration and jerk perceived by the driver within given thresholds. While functional requirements are extensively considered by the research literature, drivability requirements garner less attention.  
This industrial paper describes our experience assessing the usefulness of an automated search-based software testing (SBST) framework in generating failure-revealing test cases for functional and drivability requirements. We report on our experience with the VI-CarRealTime simulator, an industrial virtual modeling and simulation environment widely used in the automotive domain.  
We designed a Cruise Control system in Simulink for a four-wheel vehicle, in an iterative fashion, by producing 21 model versions. We used the SBST framework for each version of the model to search for failure-revealing test cases revealing requirement violations.  
Our results show that the SBST framework successfully identified a failure-revealing test case for 66.7% of our model versions, requiring, on average, 245.9s and 3.8 iterations. We present lessons learned, reflect on the generality of our results, and discuss how our results improve the state of practice.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1949–1960},
numpages = {12},
keywords = {Comfort, Cruise Control, Drivability, Model Development, Search-based Software Testing, Simulink},
location = {<conf-loc>, <city>San Francisco</city>, <state>CA</state>, <country>USA</country>, </conf-loc>},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1145/3341105.3373938,
author = {Alcon, Miguel and Tabani, Hamid and Abella, Jaume and Kosmidis, Leonidas and Cazorla, Francisco J.},
title = {En-Route: on enabling resource usage testing for autonomous driving frameworks},
year = {2020},
isbn = {9781450368667},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341105.3373938},
doi = {10.1145/3341105.3373938},
abstract = {Software resource usage testing, including execution time bounds and memory, is a mandatory validation step during the integration of safety-related real-time systems. However, the inherent complexity of Autonomous Driving (AD) systems challenges current practice for resource usage testing. This paper exposes the difficulties to perform resource usage testing for AD frameworks by analyzing a complex and critical module of an AD framework, and provides some guidelines and practical evidence on how resource usage testing can be effectively performed, thus enabling end users to validate their safety-related real-time AD frameworks.},
booktitle = {Proceedings of the 35th Annual ACM Symposium on Applied Computing},
pages = {1953–1962},
numpages = {10},
keywords = {autonomous driving systems, resource usage testing, safety-critical systems},
location = {Brno, Czech Republic},
series = {SAC '20}
}

@inproceedings{10.1145/3397125.3397137,
author = {Waheed, Fatima and Azam, Farooque and Anwar, Muhammad Waseem and Rasheed, Yawar},
title = {Model Driven Approach for Automatic Script Generation in Stress Testing of Web Applications},
year = {2020},
isbn = {9781450377492},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3397125.3397137},
doi = {10.1145/3397125.3397137},
abstract = {Mostly, software systems operate satisfactory under normal operating condition. However, if abnormal conditions are encountered i.e in the event of extraordinary load conditions, the response of the software systems may also become abnormal. Due to this reason, Stress Testing is considered an important part of testing that is used to determine the behaviour of software in extraordinary load conditions. Most of the research work that has been done in the domain of stress testing focuses on development of systems using traditional approaches of programming/coding. In this manuscript, we have anticipated an innovative approach for automatic script generation in stress testing of web applications. The proposed approach includes a meta model that may be extended for automation and model based development of such a system/ tool that may generate test scripts for stress testing of web applications using concepts of IFML.},
booktitle = {Proceedings of the 2020 6th International Conference on Computer and Technology Applications},
pages = {46–50},
numpages = {5},
keywords = {IFML, Metamodel, Model Driven Testing, Model Driven software engineering, Software testing, UML Profile},
location = {Antalya, Turkey},
series = {ICCTA '20}
}

@inproceedings{10.1145/3597503.3639109,
author = {Wang, Jun and Li, Yanhui and Chen, Zhifei and Chen, Lin and Zhang, Xiaofang and Zhou, Yuming},
title = {Knowledge Graph Driven Inference Testing for Question Answering Software},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639109},
doi = {10.1145/3597503.3639109},
abstract = {In the wake of developments in the field of Natural Language Processing, Question Answering (QA) software has penetrated our daily lives. Due to the data-driven programming paradigm, QA software inevitably contains bugs, i.e., misbehaving in real-world applications. Current testing techniques for testing QA software include two folds, reference-based testing and metamorphic testing.This paper adopts a different angle to achieve testing for QA software: we notice that answers to questions would have inference relations, i.e., the answers to some questions could be logically inferred from the answers to other questions. If these answers on QA software do not satisfy the inference relations, an inference bug is detected. To generate the questions with the inference relations automatically, we propose a novel testing method Knowledge Graph driven Inference Testing (KGIT), which employs facts in the Knowledge Graph (KG) as the seeds to logically construct test cases containing questions and contexts with inference relations. To evaluate the effectiveness of KGIT, we conduct an extensive empirical study with more than 2.8 million test cases generated from the large-scale KG YAGO4 and three QA models based on the state-of-the-art QA model structure. The experimental results show that our method (a) could detect a considerable number of inference bugs in all three studied QA models and (b) is helpful in retraining QA models to improve their inference ability.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {119},
numpages = {13},
keywords = {question answering, software testing, knowledge graph, inference rules},
location = {<conf-loc>, <city>Lisbon</city>, <country>Portugal</country>, </conf-loc>},
series = {ICSE '24}
}

@inproceedings{10.1145/3474718.3475718,
author = {Oesch, Sean and Bridges, Robert A. and Verma, Miki and Weber, Brian and Diallo, Oumar},
title = {D2U: Data Driven User Emulation for the Enhancement of Cyber Testing, Training, and Data Set Generation},
year = {2021},
isbn = {9781450390651},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474718.3475718},
doi = {10.1145/3474718.3475718},
abstract = {Whether testing intrusion detection systems, conducting training exercises, or creating data sets to be used by the broader cybersecurity community, realistic user behavior is a critical component of a cyber range. Existing methods either rely on network level data or replay recorded user actions to approximate real users in a network. Our work produces generative models trained on actual user data (sequences of application usage) collected from endpoints. Once trained to the user’s behavioral data, these models can generate novel sequences of actions from the same distribution as the training data. These sequences of actions are then fed to our custom software via configuration files, which replicate those behaviors on end devices. Notably, our models are platform agnostic and could generate behavior data for any emulation software package. In this paper we present our model generation process, software architecture, and an investigation of the fidelity of our models. Specifically, we consider two different representations of the behavioral sequences, on which three standard generative models for sequential data—Markov Chain, Hidden Markov Model, and Random Surfer—are employed. Additionally, we examine adding a latent variable to faithfully capture time-of-day trends. Best results are observed when sampling a unique next behavior (regardless of the specific sequential model used) and the duration to take the behavior, paired with the temporal latent variable. Our software is currently deployed in a cyber range to help evaluate the efficacy of defensive cyber technologies, and we suggest additional ways that the cyber community as a whole can benefit from more realistic user behavior emulation.},
booktitle = {Proceedings of the 14th Cyber Security Experimentation and Test Workshop},
pages = {17–26},
numpages = {10},
keywords = {data driven, data sets, experimental infrastructure, user emulation},
location = {Virtual, CA, USA},
series = {CSET '21}
}

@inproceedings{10.1145/3411764.3445455,
author = {Salehnamadi, Navid and Alshayban, Abdulaziz and Lin, Jun-Wei and Ahmed, Iftekhar and Branham, Stacy and Malek, Sam},
title = {Latte: Use-Case and Assistive-Service Driven Automated Accessibility Testing Framework for Android},
year = {2021},
isbn = {9781450380966},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3411764.3445455},
doi = {10.1145/3411764.3445455},
abstract = {For 15% of the world population with disabilities, accessibility is arguably the most critical software quality attribute. The ever-growing reliance of users with disability on mobile apps further underscores the need for accessible software in this domain. Existing automated accessibility assessment techniques primarily aim to detect violations of predefined guidelines, thereby produce a massive amount of accessibility warnings that often overlook the way software is actually used by users with disability. This paper presents a novel, high-fidelity form of accessibility testing for Android apps, called Latte, that automatically reuses tests written to evaluate an app’s functional correctness to assess its accessibility as well. Latte first extracts the use case corresponding to each test, and then executes each use case in the way disabled users would, i.e., using assistive services. Our empirical evaluation on real-world Android apps demonstrates Latte’s effectiveness in detecting substantially more useful defects than prior techniques.},
booktitle = {Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems},
articleno = {274},
numpages = {11},
keywords = {Accessibility, Automated Testing, Mobile Application},
location = {<conf-loc>, <city>Yokohama</city>, <country>Japan</country>, </conf-loc>},
series = {CHI '21}
}

@inproceedings{10.1145/3597503.3639149,
author = {He, Junda and Yang, Zhou and Shi, Jieke and Yang, Chengran and Kim, Kisub and Xu, Bowen and Zhou, Xin and Lo, David},
title = {Curiosity-Driven Testing for Sequential Decision-Making Process},
year = {2024},
isbn = {9798400702174},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597503.3639149},
doi = {10.1145/3597503.3639149},
abstract = {Sequential decision-making processes (SDPs) are fundamental for complex real-world challenges, such as autonomous driving, robotic control, and traffic management. While recent advances in Deep Learning (DL) have led to mature solutions for solving these complex problems, SDMs remain vulnerable to learning unsafe behaviors, posing significant risks in safety-critical applications. However, developing a testing framework for SDMs that can identify a diverse set of crash-triggering scenarios remains an open challenge. To address this, we propose CureFuzz, a novel curiosity-driven black-box fuzz testing approach for SDMs. CureFuzz proposes a curiosity mechanism that allows a fuzzer to effectively explore novel and diverse scenarios, leading to improved detection of crash-triggering scenarios. Additionally, we introduce a multi-objective seed selection technique to balance the exploration of novel scenarios and the generation of crash-triggering scenarios, thereby optimizing the fuzzing process. We evaluate CureFuzz on various SDMs and experimental results demonstrate that CureFuzz outperforms the state-of-the-art method by a substantial margin in the total number of faults and distinct types of crash-triggering scenarios. We also demonstrate that the crash-triggering scenarios found by CureFuzz can repair SDMs, highlighting CureFuzz as a valuable tool for testing SDMs and optimizing their performance.},
booktitle = {Proceedings of the IEEE/ACM 46th International Conference on Software Engineering},
articleno = {165},
numpages = {14},
keywords = {fuzz testing, sequential decision making, deep learning},
location = {<conf-loc>, <city>Lisbon</city>, <country>Portugal</country>, </conf-loc>},
series = {ICSE '24}
}

@inproceedings{10.1145/3540250.3558941,
author = {Zhu, Junjie and Long, Teng and Wang, Wei and Memon, Atif},
title = {Improving ML-based information retrieval software with user-driven functional testing and defect class analysis},
year = {2022},
isbn = {9781450394130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3540250.3558941},
doi = {10.1145/3540250.3558941},
abstract = {Machine Learning (ML) has become the cornerstone of information retrieval (IR) software, as it can drive better user experience by leveraging information-rich data and complex models. However, evaluating the emergent behavior of ML-based IR software can be challenging with traditional software testing approaches: when developers modify the software, they cannot often extract useful information from individual test instances; rather, they seek to holistically verify whether—and where—their modifications caused significant regressions or improvements at scale. In this paper, we introduce not only such a holistic approach to evaluate the system-level behavior of the software, but also the concept of a defect class, which represents a partition of the input space on which the ML-based software does measurably worse for an existing feature or on which the ML task is more challenging for a new feature. We leverage large volumes of functional test cases, automatically obtained, to derive these defect classes, and propose new ways to improve the IR software from an end-user’s perspective. Applying our approach on a real production Search-AutoComplete system that contains a query interpretation ML component, we demonstrate that (1) our holistic metrics successfully identified two regressions and one improvement, where all 3 were independently verified with retrospective A/B experiments, (2) the automatically obtained defect classes provided actionable insights during early-stage ML development, and (3) we also detected defect classes at the finer sub-component level for which there were significant regressions, which we blocked prior to different releases.},
booktitle = {Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1291–1301},
numpages = {11},
keywords = {AutoComplete Search, Information Retrieval System Testing, Machine Learning Testing, Query Interpretation, Relevance Search},
location = {<conf-loc>, <city>Singapore</city>, <country>Singapore</country>, </conf-loc>},
series = {ESEC/FSE 2022}
}

@inproceedings{10.1109/ICSE43902.2021.00048,
author = {Zheng, Yan and Liu, Yi and Xie, Xiaofei and Liu, Yepang and Ma, Lei and Hao, Jianye and Liu, Yang},
title = {Automatic Web Testing Using Curiosity-Driven Reinforcement Learning},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00048},
doi = {10.1109/ICSE43902.2021.00048},
abstract = {Web testing has long been recognized as a notoriously difficult task. Even nowadays, web testing still heavily relies on manual efforts while automated web testing is far from achieving human-level performance. Key challenges in web testing include dynamic content update and deep bugs hiding under complicated user interactions and specific input values, which can only be triggered by certain action sequences in the huge search space. In this paper, we propose WebExplor, an automatic end-to-end web testing framework, to achieve an adaptive exploration of web applications. WebExplor adopts curiosity-driven reinforcement learning to generate high-quality action sequences (test cases) satisfying temporal logical relations. Besides, WebExplor incrementally builds an automaton during the online testing process, which provides high-level guidance to further improve the testing efficiency. We have conducted comprehensive evaluations of WebExplor on six real-world projects, a commercial SaaS web application, and performed an in-the-wild study of the top 50 web applications in the world. The results demonstrate that in most cases WebExplor can achieve significantly higher failure detection rate, code coverage and efficiency than existing state-of-the-art web testing techniques. WebExplor also detected 12 previously unknown failures in the commercial web application, which have been confirmed and fixed by the developers. Furthermore, our in-the-wild study further uncovered 3,466 exceptions and errors.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {423–435},
numpages = {13},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{10.1109/ASP-DAC58780.2024.10473874,
author = {Saha, Anurup and Amarnath, Chandramouli and Ma, Kwondo and Chatterjee, Abhijit},
title = {Signature Driven Post-Manufacture Testing and Tuning of RRAM Spiking Neural Networks for Yield Recovery},
year = {2024},
isbn = {9798350393545},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASP-DAC58780.2024.10473874},
doi = {10.1109/ASP-DAC58780.2024.10473874},
abstract = {Resistive random access Memory (RRAM) based spiking neural networks (SNN) are becoming increasingly attractive for pervasive energy-efficient classification tasks. However, such networks suffer from degradation of performance (as determined by classification accuracy) due to the effects of process variations on fabricated RRAM devices resulting in loss of manufacturing yield. To address such yield loss, a two-step approach is developed. First, an alternative test framework is used to predict the performance of fabricated RRAM based SNNs using the SNN response to a small subset of images from the test image dataset, called the SNN response signature (to minimize test cost). This diagnoses those SNNs that need to be performance-tuned for yield recovery. Next, SNN tuning is performed by modulating the spiking thresholds of the SNN neurons on a layer-by-layer basis using a trained regressor that maps the SNN response signature to the optimal spiking threshold values during tuning. The optimal spiking threshold values are determined by an off-line optimization algorithm. Experiments show that the proposed framework can reduce the number of out-of-spec SNN devices by up to 54% and improve yield by as much as 8.6%.},
booktitle = {Proceedings of the 29th Asia and South Pacific Design Automation Conference},
pages = {740–745},
numpages = {6},
keywords = {spiking neural network, yield recovery, alternative test, post-manufacture tuning},
location = {<conf-loc>, <city>Incheon</city>, <country>Republic of Korea</country>, </conf-loc>},
series = {ASPDAC '24}
}

@article{10.1145/3632904,
author = {Sotiropoulos, Thodoris and Chaliasos, Stefanos and Su, Zhendong},
title = {API-Driven Program Synthesis for Testing Static Typing Implementations},
year = {2024},
issue_date = {January 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {POPL},
url = {https://doi.org/10.1145/3632904},
doi = {10.1145/3632904},
abstract = {We introduce a novel approach for testing static typing implementations based on the concept of API-driven program synthesis. The idea is to synthesize type-intensive but small and well-typed programs by leveraging and combining application programming interfaces (APIs) derived from existing software libraries. Our primary insight is backed up by real-world evidence: a significant number of compiler typing bugs are caused by small test cases that employ APIs from the standard library of the language under test. This is attributed to the inherent complexity of the majority of these APIs, which often exercise a wide range of sophisticated type-related features. The main contribution of our approach is the ability to produce small client programs with increased feature coverage, without bearing the burden of generating the corresponding well-formed API definitions from scratch. To validate diverse aspects of static typing procedures (i.e., soundness, precision of type inference), we also enrich our API-driven approach with fault-injection and semantics-preserving modes, along with their corresponding test oracles.  

We evaluate our implemented tool, Thalia on testing the static typing implementations of the compilers for three popular languages, namely, Scala, Kotlin, and Groovy. Thalia has uncovered 84 typing bugs (77 confirmed and 22 fixed), most of which are triggered by test cases featuring APIs that rely on parametric polymorphism, overloading, and higher-order functions. Our comparison with state-of-the-art shows that Thalia yields test programs with distinct characteristics, offering additional and complementary benefits.},
journal = {Proc. ACM Program. Lang.},
month = {jan},
articleno = {62},
numpages = {32},
keywords = {API, compiler bug, compiler testing, enumeration, library, type system}
}

@inproceedings{10.1145/3582016.3582053,
author = {Li, Shaohua and Su, Zhendong},
title = {Finding Unstable Code via Compiler-Driven Differential Testing},
year = {2023},
isbn = {9781450399180},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582016.3582053},
doi = {10.1145/3582016.3582053},
abstract = {Unstable code refers to code that has inconsistent or unstable run-time semantics due to undefined behavior (UB) in the program. Compilers exploit UB by assuming that UB never occurs, which allows them to generate efficient but potentially semantically inconsistent binaries. Practitioners have put great research and engineering effort into designing dynamic tools such as sanitizers for frequently occurring UBs. However, it remains a big challenge how to detect UBs that are beyond the reach of current techniques. In this paper, we introduce compiler-driven differential testing (CompDiff), a simple yet effective approach for finding unstable code in C/C++ programs. CompDiff relies on the fact that when compiling unstable code, different compiler implementations may produce semantically inconsistent binaries. Our main approach is to examine the outputs of different binaries on the same input. Discrepancies in outputs may signify the existence of unstable code. To detect unstable code in real-world programs, we also integrate CompDiff into AFL++, the most widely-used and actively-maintained general-purpose fuzzer. Despite its simplicity, CompDiff is effective in practice: on the Juliet benchmark programs, CompDiff uniquely detected 1,409 bugs compared to sanitizers; on 23 popular open-source C/C++ projects, CompDiff-AFL++ uncovered 78 new bugs, 52 of which have been fixed by developers and 36 cannot be detected by sanitizers. Our evaluation also reveals the fact that CompDiff is not designed to replace current UB detectors but to complement them.},
booktitle = {Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
pages = {238–251},
numpages = {14},
keywords = {compiler, undefined behavior, unstable code},
location = {<conf-loc>, <city>Vancouver</city>, <state>BC</state>, <country>Canada</country>, </conf-loc>},
series = {ASPLOS 2023}
}

@inproceedings{10.1145/3377816.3381742,
author = {Terragni, Valerio and Salza, Pasquale and Ferrucci, Filomena},
title = {A container-based infrastructure for fuzzy-driven root causing of flaky tests},
year = {2020},
isbn = {9781450371261},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377816.3381742},
doi = {10.1145/3377816.3381742},
abstract = {Intermittent test failures (test flakiness) is common during continuous integration as modern software systems have become inherently non-deterministic. Understanding the root cause of test flakiness is crucial as intermittent test failures might be the result of real non-deterministic defects in the production code, rather than mere errors in the test code. Given a flaky test, existing techniques for root causing test flakiness compare the runtime behavior of its passing and failing executions. They achieve this by repetitively executing the flaky test on an instrumented version of the system under test. This approach has two fundamental limitations: (i) code instrumentation might prevent the manifestation of test flakiness; (ii) when test flakiness is rare passively re-executing a test many times might be inadequate to trigger intermittent test outcomes. To address these limitations, we propose a new idea for root causing test flakiness that actively explores the non-deterministic space without instrumenting code. Our novel idea is to repetitively execute a flaky test, under different execution clusters. Each cluster explores a certain non-deterministic dimension (e.g., concurrency, I/O, and networking) with dedicated software containers and fuzzy-driven resource load generators. The execution cluster that manifests the most balanced (or unbalanced) sets of passing and failing executions is likely to explain the broad type of test flakiness.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {69–72},
numpages = {4},
keywords = {cloud, concurrency, fuzzy analysis, non-determinism, root-causing analysis, software containers, test flakiness},
location = {Seoul, South Korea},
series = {ICSE-NIER '20}
}

@article{10.1145/3554726,
author = {Battle, Leilani},
title = {Behavior-driven testing of big data exploration tools},
year = {2022},
issue_date = {September - October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {5},
issn = {1072-5520},
url = {https://doi.org/10.1145/3554726},
doi = {10.1145/3554726},
journal = {Interactions},
month = {aug},
pages = {9–10},
numpages = {2}
}

@article{10.1145/3502297,
author = {Ngo, Chanh Duc and Pastore, Fabrizio and Briand, Lionel},
title = {Automated, Cost-effective, and Update-driven App Testing},
year = {2022},
issue_date = {October 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3502297},
doi = {10.1145/3502297},
abstract = {Apps’ pervasive role in our society led to the definition of test automation approaches to ensure their dependability. However, state-of-the-art approaches tend to generate large numbers of test inputs and are unlikely to achieve more than 50% method coverage.In this article, we propose a strategy to achieve significantly higher coverage of the code affected by updates with a much smaller number of test inputs, thus alleviating the test oracle problem.More specifically, we present ATUA, a model-based approach that synthesizes App models with static analysis, integrates a dynamically refined state abstraction function and combines complementary testing strategies, including (1) coverage of the model structure, (2) coverage of the App code, (3) random exploration, and (4) coverage of dependencies identified through information retrieval. Its model-based strategy enables ATUA to generate a small set of inputs that exercise only the code affected by the updates. In turn, this makes common test oracle solutions more cost-effective, as they tend to involve human effort.A large empirical evaluation, conducted with 72 App versions belonging to nine popular Android Apps, has shown that ATUA is more effective and less effort-intensive than state-of-the-art approaches when testing App updates.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {jul},
articleno = {61},
numpages = {51},
keywords = {Android testing, regression testing, upgrade testing, model-based testing, information retrieval}
}

@inproceedings{10.1109/ICSE-SEIP52600.2021.00026,
author = {Li, Paul Luo and Chai, Xiaoyu and Campbell, Frederick and Liao, Jilong and Abburu, Neeraja and Kang, Minsuk and Niculescu, Irina and Brake, Greg and Patil, Siddharth and Dooley, James and Paddock, Brandon},
title = {Evolving software to be ML-driven utilizing real-world A/B testing: experiences, insights, challenges},
year = {2021},
isbn = {9780738146690},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP52600.2021.00026},
doi = {10.1109/ICSE-SEIP52600.2021.00026},
abstract = {ML-driven software is heralded as the next major advancement in software engineering; existing software today can benefit from being evolved to be ML-driven. In this paper, we contribute practical knowledge about evolving software to be ML-driven, utilizing real-world A/B testing. We draw on experiences evolving two software features from the Windows operating system to be ML-driven, with more than ten realworld A/B tests on millions of PCs over more than two years. We discuss practical reasons for using A/B testing to engineer ML-driven software, insights for success, as well as on-going realworld challenges. This knowledge may help practitioners, as well as help direct future research and innovations.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Software Engineering in Practice},
pages = {170–179},
numpages = {10},
keywords = {big data applications, data analysis, learning (artificial intelligence), machine learning, machine learning algorithms, predictive models, software development management, software engineering, software quality},
location = {Virtual Event, Spain},
series = {ICSE-SEIP '21}
}

@article{10.1145/3624745,
author = {Formica, Federico and Fan, Tony and Menghi, Claudio},
title = {Search-Based Software Testing Driven by Automatically Generated and Manually Defined Fitness Functions},
year = {2023},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3624745},
doi = {10.1145/3624745},
abstract = {Search-based software testing (SBST) typically relies on fitness functions to guide the search exploration toward software failures. There are two main techniques to define fitness functions: (a)&nbsp;automated fitness function computation from the specification of the system requirements, and (b)&nbsp;manual fitness function design. Both techniques have advantages. The former uses information from the system requirements to guide the search toward portions of the input domain more likely to contain failures. The latter uses the engineers’ domain knowledge.We propose ATheNA, a novel SBST framework that combines fitness functions automatically generated from requirements specifications and those manually defined by engineers. We design and implement ATheNA-S, an instance of ATheNA that targets Simulink® models. We evaluate ATheNA-S by considering a large set of models from different domains. Our results show that ATheNA-S generates more failure-revealing test cases than existing baseline tools and that the difference between the runtime performance of ATheNA-S and the baseline tools is not statistically significant. We also assess whether ATheNA-S could generate failure-revealing test cases when applied to two representative case studies: one from the automotive domain and one from the medical domain. Our results show that ATheNA-S successfully revealed a requirement violation in our case studies.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {dec},
articleno = {40},
numpages = {37},
keywords = {Testing, falsification, fitness functions, CPS}
}

@inproceedings{10.1145/3524482.3527657,
author = {Sharma, Arnab and Melnikov, Vitalik and H\"{u}llermeier, Eyke and Wehrheim, Heike},
title = {Property-driven testing of black-box functions},
year = {2022},
isbn = {9781450392877},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524482.3527657},
doi = {10.1145/3524482.3527657},
abstract = {Testing is one of the most frequent means of quality assurance for software. Property-based testing aims at generating test suites for checking code against user-defined properties. Test input generation is, however, most often independent of the property to be checked, and is instead based on random or user-defined data generation.In this paper, we present property-driven unit testing of functions with numerical inputs and outputs. Alike property-based testing, it allows users to define the properties to be tested for. Contrary to property-based testing, it also uses the property for a targeted generation of test inputs. Our approach is a form of learning-based testing where we first of all learn a model of a given black-box function using standard machine learning algorithms, and in a second step use model and property for test input generation. This allows us to test both predefined functions as well as machine learned regression models. Our experimental evaluation shows that our property-driven approach is more effective than standard property-based testing techniques.},
booktitle = {Proceedings of the IEEE/ACM 10th International Conference on Formal Methods in Software Engineering},
pages = {113–123},
numpages = {11},
keywords = {machine learning, property-based testing, regression},
location = {Pittsburgh, Pennsylvania},
series = {FormaliSE '22}
}

@inproceedings{10.1145/3533767.3543293,
author = {Ngo, Chanh-Duc and Pastore, Fabrizio and Briand, Lionel C.},
title = {ATUA: an update-driven app testing tool},
year = {2022},
isbn = {9781450393799},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3533767.3543293},
doi = {10.1145/3533767.3543293},
abstract = {App testing tools tend to generate thousand test inputs; they help engineers identify crashing conditions but not functional failures. Indeed, detecting functional failures requires the visual inspection of App outputs, which is infeasible for thousands of inputs.  
Existing App testing tools ignore that most of the Apps are frequently updated and engineers are mainly interested in testing the updated functionalities; indeed, automated regression test cases can be used otherwise.  
We present ATUA, an open source tool targeting Android Apps. It achieves high coverage of the updated App code with a small number of test inputs, thus alleviating the test oracle problem (less outputs to inspect). It implements a model-based approach that synthesizes App models with static analysis, integrates a dynamically-refined state abstraction function and combines complementary testing strategies, including (1) coverage of the model structure, (2) coverage of the App code, (3) random exploration, and (4) coverage of dependencies identified through information retrieval.  
Our empirical evaluation, conducted with nine popular Android Apps (72 versions), has shown that ATUA, compared to state-of-the-art approaches, achieves higher code coverage while producing fewer outputs to be manually inspected.  
A demo video is available at https://youtu.be/RqQ1z_Nkaqo.},
booktitle = {Proceedings of the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {765–768},
numpages = {4},
keywords = {Android Testing, Regression Testing, Upgrade Testing},
location = {<conf-loc>, <city>Virtual</city>, <country>South Korea</country>, </conf-loc>},
series = {ISSTA 2022}
}

@inproceedings{10.1145/3395363.3397354,
author = {Pan, Minxue and Huang, An and Wang, Guoxin and Zhang, Tian and Li, Xuandong},
title = {Reinforcement learning based curiosity-driven testing of Android applications},
year = {2020},
isbn = {9781450380089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395363.3397354},
doi = {10.1145/3395363.3397354},
abstract = {Mobile applications play an important role in our daily life, while it still remains a challenge to guarantee their correctness. Model-based and systematic approaches have been applied to Android GUI testing. However, they do not show significant advantages over random approaches because of limitations such as imprecise models and poor scalability. In this paper, we propose Q-testing, a reinforcement learning based approach which benefits from both random and model-based approaches to automated testing of Android applications. Q-testing explores the Android apps with a curiosity-driven strategy that utilizes a memory set to record part of previously visited states and guides the testing towards unfamiliar functionalities. A state comparison module, which is a neural network trained by plenty of collected samples, is novelly employed to divide different states at the granularity of functional scenarios. It can determine the reinforcement learning reward in Q-testing and help the curiosity-driven strategy explore different functionalities efficiently. We conduct experiments on 50 open-source applications where Q-testing outperforms the state-of-the-art and state-of-practice Android GUI testing tools in terms of code coverage and fault detection. So far, 22 of our reported faults have been confirmed, among which 7 have been fixed.},
booktitle = {Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {153–164},
numpages = {12},
keywords = {Android app testing, functional scenario division, reinforcement learning},
location = {Virtual Event, USA},
series = {ISSTA 2020}
}

@article{10.1109/TNET.2021.3077652,
author = {Zhang, Dai and Zhou, Yu and Xi, Zhaowei and Wang, Yangyang and Xu, Mingwei and Wu, Jianping},
title = {HyperTester: High-Performance Network Testing Driven by Programmable Switches},
year = {2021},
issue_date = {Oct. 2021},
publisher = {IEEE Press},
volume = {29},
number = {5},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2021.3077652},
doi = {10.1109/TNET.2021.3077652},
abstract = {Modern network devices and systems are raising higher requirements on network testers that are regularly used to evaluate performance and assess correctness. These requirements include high scale, high accuracy, flexibility and low cost, which existing testers cannot fulfill at the same time. In this paper, we propose HyperTester, a network tester leveraging new-generation programmable switches and achieving all of the above goals simultaneously. Programmable switches are born with features like high throughput and linerate, deterministic processing pipelines and nanosecond-level hardware timestamps, the P4 programming model as well as comparable pricing with commodity servers, but they come with limited programmability and memory resources. HyperTester uses template-based packet generation to overcome the limitations of the switch ASIC in programmability and designs a stateless connection mechanism as well as counter-based state compression algorithms to overcome the memory resource constraints in the data plane. We have implemented HyperTester on Tofino, and the evaluations on the hardware testbed show that HyperTester supports high-scale packet generation (more than 1.6Tbps) and achieves highly accurate rate control and timestamping. We demonstrate that programmable switches can be potential and attractive targets for realizing network testers.},
journal = {IEEE/ACM Trans. Netw.},
month = {may},
pages = {2005–2018},
numpages = {14}
}

@inproceedings{10.1145/3393822.3432338,
author = {Hosobe, Hiroshi},
title = {Testing Event-Driven Programs in Processing},
year = {2020},
isbn = {9781450377621},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3393822.3432338},
doi = {10.1145/3393822.3432338},
abstract = {Event-driven programming is a paradigm that is widely used in many fields. Processing is a set of programming languages and environments specialized in event-driven programming for interactive graphical applications. It provides only low-level event-handling functions, which imposes difficulty on novice programmers in programming complex behaviors. This paper proposes a method for unit-testing event-driven Processing programs. It allows writing testable Processing programs and test programs in Java. To demonstrate how it works, this paper presents case studies on testing whether mouse and key events are correctly handled.},
booktitle = {Proceedings of the 2020 European Symposium on Software Engineering},
pages = {6–11},
numpages = {6},
keywords = {Processing, Unit testing, event-driven programming},
location = {Rome, Italy},
series = {ESSE '20}
}

@inproceedings{10.1145/3377812.3381388,
author = {Martin-Lopez, Alberto},
title = {AI-driven web API testing},
year = {2020},
isbn = {9781450371223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377812.3381388},
doi = {10.1145/3377812.3381388},
abstract = {Testing of web APIs is nowadays more critical than ever before, as they are the current standard for software integration. A bug in an organization's web API could have a huge impact both internally (services relying on that API) and externally (third-party applications and end users). Most existing tools and testing approaches require writing tests or instrumenting the system under test (SUT). The main aim of this dissertation is to take web API testing to an unprecedented level of automation and thoroughness. To this end, we plan to apply artificial intelligence (AI) techniques for the autonomous detection of software failures. Specifically, the idea is to develop intelligent programs (we call them "bots") capable of generating hundreds, thousands or even millions of test inputs and to evaluate whether the test outputs are correct based on: 1) patterns learned from previous executions of the SUT; and 2) knowledge gained from analyzing thousands of similar programs. Evaluation results of our initial prototype are promising, with bugs being automatically detected in some real-world APIs.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Companion Proceedings},
pages = {202–205},
numpages = {4},
keywords = {artificial intelligence, automated software testing, restful API, testing framework, web API},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1145/3503229.3547048,
author = {Prikler, Liliana Marie and Wotawa, Franz},
title = {Challenges of testing self-adaptive systems},
year = {2022},
isbn = {9781450392068},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503229.3547048},
doi = {10.1145/3503229.3547048},
abstract = {Self-adaptive systems can change their behavior due to internal or external issues detected during operation. Such systems should be able to change their internal structure or functionality to cope with broken motors or changes in the infrastructure. Assuring that the adaptations taken during operation do not impact the desired behavior or functionality of the system is of uttermost interest. In this paper, we contribute to the corresponding quality assurance challenge. In particular, we focus on a specific class of self-adaptive systems utilizing health states for computing repair actions. We discuss the requirements of testing methodologies for such systems and raise relevant research questions.},
booktitle = {Proceedings of the 26th ACM International Systems and Software Product Line Conference - Volume B},
pages = {224–228},
numpages = {5},
keywords = {quality assurance, self-adaptive systems, self-healing, testing},
location = {Graz, Austria},
series = {SPLC '22}
}

@article{10.1145/3460959,
author = {Langford, Michael Austin and Cheng, Betty H. C.},
title = {Enki: A Diversity-driven Approach to Test and Train Robust Learning-enabled Systems},
year = {2021},
issue_date = {June 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {15},
number = {2},
issn = {1556-4665},
url = {https://doi.org/10.1145/3460959},
doi = {10.1145/3460959},
abstract = {Data-driven Learning-enabled Systems are limited by the quality of available training data, particularly when trained offline. For systems that must operate in real-world environments, the space of possible conditions that can occur is vast and difficult to comprehensively predict at design time. Environmental uncertainty arises when run-time conditions diverge from design-time training conditions. To address this problem, automated methods can generate synthetic data to fill in gaps for training and test data coverage. We propose an evolution-based technique to assist developers with uncovering limitations in existing data when previously unseen environmental phenomena are introduced. This technique explores unique contexts for a given environmental condition, with an emphasis on diversity. Synthetic data generated by this technique may be used for two purposes: (1) to assess the robustness of a system to uncertain environmental factors and (2) to improve the system’s robustness. This technique is demonstrated to outperform random and greedy methods for multiple adverse environmental conditions applied to image-processing Deep Neural Networks.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = {may},
articleno = {5},
numpages = {32},
keywords = {Evolutionary computation, novelty search, software assurance, uncertainty}
}

@inproceedings{10.1145/3399715.3399862,
author = {Achberger, Alexander and Cutura, Ren\'{e} and T\"{u}rksoy, Oguzhan and Sedlmair, Michael},
title = {Caarvida: Visual Analytics for Test Drive Videos},
year = {2020},
isbn = {9781450375351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3399715.3399862},
doi = {10.1145/3399715.3399862},
abstract = {We report on an interdisciplinary visual analytics project wherein automotive engineers analyze test drive videos. These videos are annotated with navigation-specific augmented reality (AR) content, and the engineers need to identify issues and evaluate the behavior of the underlying AR navigation system. With the increasing amount of video data, traditional analysis approaches can no longer be conducted in an acceptable timeframe. To address this issue, we collaboratively developed Caarvida, a visual analytics tool that helps engineers to accomplish their tasks faster and handle an increased number of videos. Caarvida combines automatic video analysis with interactive and visual user interfaces. We conducted two case studies which show that Caarvida successfully supports domain experts and speeds up their task completion time.},
booktitle = {Proceedings of the International Conference on Advanced Visual Interfaces},
articleno = {6},
numpages = {9},
keywords = {automotive, human computer interaction, information visualization, object detection, visual analytics},
location = {Salerno, Italy},
series = {AVI '20}
}

@article{10.1145/3369838,
author = {Kim, Yoonji and Choi, Youngkyung and Kang, Daye and Lee, Minkyeong and Nam, Tek-Jin and Bianchi, Andrea},
title = {HeyTeddy: Conversational Test-Driven Development for Physical Computing},
year = {2020},
issue_date = {December 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {4},
url = {https://doi.org/10.1145/3369838},
doi = {10.1145/3369838},
abstract = {Physical computing is a complex activity that consists of different but tightly coupled tasks: programming and assembling hardware for circuits. Prior work clearly shows that this coupling is the main source of mistakes that unfruitfully take a large portion of novices' debugging time. While past work presented systems that simplify prototyping or introduce novel debugging functionalities, these tools either limit what users can accomplish or are too complex for beginners. In this paper, we propose a general-purpose prototyping tool based on conversation. HeyTeddy guides users during hardware assembly by providing additional information on requests or by interactively presenting the assembly steps to build a circuit. Furthermore, the user can program and execute code in real-time on their Arduino platform without having to write any code, but instead by using commands triggered by voice or text via chat. Finally, the system also presents a set of test capabilities for enhancing debugging with custom and proactive unit tests. We codesigned the system with 10 users over 6 months and tested it with realistic physical computing tasks. With the result of two user studies, we show that conversational programming is feasible and that voice is a suitable alternative for programming simple logic and encouraging exploration. We also demonstrate that conversational programming with unit tests is effective in reducing development time and overall debugging problems while increasing users' confidence. Finally, we highlight limitations and future avenues of research.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {sep},
articleno = {139},
numpages = {21},
keywords = {Conversational agent, End-user development, Physical computing, Test-driven development}
}

@inproceedings{10.1109/ASE.2019.00013,
author = {Gladisch, Christoph and Heinz, Thomas and Heinzemann, Christian and Oehlerking, Jens and von Vietinghoff, Anne and Pfitzer, Tim},
title = {Experience paper: search-based testing in automated driving control applications},
year = {2020},
isbn = {9781728125084},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2019.00013},
doi = {10.1109/ASE.2019.00013},
abstract = {Automated test generation and evaluation in simulation environments is a key technology for verification of automated driving (AD) applications. Search-based testing (SBT) is an approach for automated test generation that leverages optimization to efficiently generate interesting concrete tests from abstract test descriptions. In this experience paper, we report on our observations after successfully applying SBT to AD control applications in several use cases with different characteristics. Based on our experiences, we derive a number of lessons learned that we consider important for the adoption of SBT methods and tools in industrial settings. The key lesson is that SBT finds relevant errors and provides valuable feedback to the developers, but requires tool support for writing specifications.},
booktitle = {Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering},
pages = {26–37},
numpages = {12},
keywords = {automated driving, automated test generation, experience paper, search-based testing},
location = {San Diego, California},
series = {ASE '19}
}

@inproceedings{10.1109/ASE.2019.00103,
author = {Yu, Shengcheng and Fang, Chunrong and Feng, Yang and Zhao, Wenyuan and Chen, Zhenyu},
title = {LIRAT: layout and image recognition driving automated mobile testing of cross-platform},
year = {2020},
isbn = {9781728125084},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2019.00103},
doi = {10.1109/ASE.2019.00103},
abstract = {The fragmentation issue spreads over multiple mobile platforms such as Android, iOS, mobile web, and WeChat, which hinders test scripts from running across platforms. To reduce the cost of adapting scripts for various platforms, some existing tools apply conventional computer vision techniques to replay the same script on multiple platforms. However, because these solutions can hardly identify dynamic or similar widgets. It becomes difficult for engineers to apply them in practice.In this paper, we present an image-driven tool, namely LIRAT, to record and replay test scripts cross platforms, solving the problem of test script cross-platform replay for the first time. LIRAT records screenshots and layouts of the widgets, and leverages image understanding techniques to locate them in the replay process. Based on accurate widget localization, LIRAT supports replaying test scripts across devices and platforms. We employed LIRAT to replay 25 scripts from 5 application across 8 Android devices and 2 iOS devices. The results show that LIRAT can replay 88% scripts on Android platforms and 60% on iOS platforms. The demo can be found at: https://github.com/YSC9848/LIRAT},
booktitle = {Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1066–1069},
numpages = {4},
keywords = {cross-platform testing, image recognition, record and replay},
location = {San Diego, California},
series = {ASE '19}
}

@inproceedings{10.1145/3426746.3434061,
author = {Shiiba, Ryusei and Nakamura, Ryo and Kuga, Yohei and Suzuki, Shigeya},
title = {Quickly Testing NIC and Driver Interactions by a Software-based NIC},
year = {2020},
isbn = {9781450381833},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3426746.3434061},
doi = {10.1145/3426746.3434061},
abstract = {To derive the performance of recent high-speed Ethernet NICs, interaction between NIC hardware and its driver over PCIe is one of the important design spaces. However, tackling this space is still challenging. Prototyping NICs with hardware requires significant development efforts. Prototyping NICs in software has productivity compared with the hardware ones; however, it does not involve PCIe on which the communication between the driver and device is based. To achieve high productivity by software and prototyping with actual PCIe simultaneously, we propose a development platform for Ethernet NICs in software, but connected with hardware PCIe elements. In this platform, researchers and developers can easily prototype new designs of interaction between an Ethernet device in software and a driver for a hardware NIC over the physical PCIe links. To demonstrate the platform, we prototyped a new interaction for low latency packet transmission on the platform. This demonstration indicates that the platform enables quickly testing the new interactions.},
booktitle = {Proceedings of the Student Workshop},
pages = {21–22},
numpages = {2},
keywords = {Ethernet NIC, PCIe},
location = {Barcelona, Spain},
series = {CoNEXT'20}
}

@inproceedings{10.1145/3374549.3374572,
author = {Luengruengroj, Peerawut and Suwannasart, Taratip},
title = {Stubs and Drivers Generator for Class Integration Testing Using Sequence and Class Diagrams},
year = {2020},
isbn = {9781450376495},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3374549.3374572},
doi = {10.1145/3374549.3374572},
abstract = {This paper aims to present an upgraded "Stubs and Drivers Generating Tool" from our previous research [1] which proposed a web-application for generating stubs and drivers for unit testing. However, the tool has limitations in generating stubs and drivers from a class diagram with abstraction and it allows a tester to select only one class under test for generating them while generated drivers do not include test input values. This enhanced tool generates stubs and drivers with test input values using UML sequence and class diagrams. A tester can perform class integration testing automation with the tool by importing both sequence and class diagrams in XML format. Then, extracting and analyzing elements to produce a call graph. The tester can select a group of classes under test. After that, the tool generates a set of stub and driver files for class integration testing. The tool also randomly generates test input values and expected result for drivers. Finally, the tester can customize the source code generated from the tool and export these files for using in class integration testing process.},
booktitle = {Proceedings of the 2019 3rd International Conference on Software and E-Business},
pages = {115–119},
numpages = {5},
keywords = {Class Integration Testing, Driver, Object-Oriented, Sequence Diagram, Software Testing, Stub, Test Case},
location = {Tokyo, Japan},
series = {ICSEB '19}
}

@inproceedings{10.1145/3377811.3380431,
author = {Qian, Ju and Shang, Zhengyu and Yan, Shuoyan and Wang, Yan and Chen, Lin},
title = {RoScript: a visual script driven truly non-intrusive robotic testing system for touch screen applications},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380431},
doi = {10.1145/3377811.3380431},
abstract = {Existing intrusive test automation techniques for touch screen applications (e.g., Appium and Sikuli) are difficult to work on many closed or uncommon systems, such as a GoPro. Being non-intrusive can largely extend the application scope of the test automation techniques. To this end, this paper presents RoScript, a truly non-intrusive test-script-driven robotic testing system for test automation of touch screen applications. RoScript leverages visual test scripts to express GUI actions on a touch screen application and uses a physical robot to drive automated test execution. To reduce the test script creation cost, a non-intrusive computer vision based technique is also introduced in RoScript to automatically record touch screen actions into test scripts from videos of human actions on the device under test. RoScript is applicable to touch screen applications running on almost arbitrary platforms, whatever the underlying operating systems or GUI frameworks are. We conducted experiments applying it to automate the testing of 21 touch screen applications on 6 different devices. The results show that RoScript is highly usable. In the experiments, it successfully automated 104 test scenarios containing over 650 different GUI actions on the subject applications. RoScript accurately performed GUI actions on over 90% of the test script executions and accurately recorded about 85% of human screen click actions into test code.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {297–308},
numpages = {12},
keywords = {GUI testing, computer vision, non-intrusive, robot, test automation},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1145/3579375.3579412,
author = {Hua, David and Petrina, Neysa and Young, Noel and Cho, Jin-Gun and Sacks, Alan and Poon, Simon},
title = {Using AI-Driven Triaging to Optimise Clinical Workflows in Non-Emergency Outpatient Settings: A Real-World Case Study Concerning the Screening of Tuberculosis},
year = {2023},
isbn = {9798400700057},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3579375.3579412},
doi = {10.1145/3579375.3579412},
abstract = {Growing diagnostic imaging workloads threaten to undermine the sustainable delivery of healthcare imaging services, worsening clinical and productivity outcomes. Artificial intelligence (AI) technology promises to help address this issue through its triaging capabilities which can optimise clinical workflows and patient management. However, the impact and trade-offs of AI-driven triaging must be understood to determine whether it is a net positive and more beneficial than a first-in-first-out queueing approach. Existing research has focused on studying the impacts of AI-driven triaging in emergency department contexts and has given less attention to outpatient settings. This research in progress paper presents the preliminary results of an industry-academia collaboration study exploring the real-world impact of AI-driven triaging in the diagnosis of tuberculosis (TB) for an outpatient setting. A mixed-methods approach is adopted to examine radiologist perspectives of its effect on daily clinical practice and to quantify its productivity impact on the time spent completing cases. Further investigation to establish the impact of AI-driven triaging on the diagnostic accuracy of clinicians and its relationship with task productivity is underway.},
booktitle = {Proceedings of the 2023 Australasian Computer Science Week},
pages = {240–243},
numpages = {4},
keywords = {AI-assisted decision-making, artificial intelligence, diagnostic imaging, optimised clinical workflows, task productivity, triaging},
location = {<conf-loc>, <city>Melbourne</city>, <state>VIC</state>, <country>Australia</country>, </conf-loc>},
series = {ACSW '23}
}

@inproceedings{10.1145/3473856.3474015,
author = {Kopka, Marvin and Krause, Karen},
title = {Can You Help Me? Testing HMI Designs and Psychological Influences on Intended Helping Behavior Towards Autonomous Cargo Bikes},
year = {2021},
isbn = {9781450386456},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3473856.3474015},
doi = {10.1145/3473856.3474015},
abstract = {Autonomous (cargo-)bikes offer many use cases, especially in urban areas. One challenge they have to face is their dependence on human assistance. This study examines the influence of light color, flashing rhythm, voices, a person’s kindness and technological experience on helping behavior towards an autonomous cargo bike. An experiment with 233 participants was conducted. We found that technological experience and kindness had a positive influence on helping behavior, while light color, flashing rhythm and voice type did not have any influence. Our results imply that helping behavior towards an autonomous bicycle varies individually. However, an electronically generated voice should be used to make the vehicle appear autonomous, emphasize the absence of a human being and give clear instructions to promote helping behavior.},
booktitle = {Proceedings of Mensch Und Computer 2021},
pages = {64–68},
numpages = {5},
keywords = {GACS-72, HMI design, autonomous vehicles, external HMI, helping behavior, human factors, human machine interface, psychology, unmanned vehicles, vehicle pedestrian interaction},
location = {Ingolstadt, Germany},
series = {MuC '21}
}

@inproceedings{10.1145/3555050.3569124,
author = {Sun, Chuanhao and Xu, Kai and Marina, Mahesh K. and Benn, Howard},
title = {GenDT: mobile network drive testing made efficient with generative modeling},
year = {2022},
isbn = {9781450395083},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3555050.3569124},
doi = {10.1145/3555050.3569124},
abstract = {Drive testing continues to play a key role in mobile network optimization for operators but its high cost is a big concern. Alternative approaches like virtual drive testing (VDT) target device testing in the lab whereas MDT or crowdsourcing based approaches are limited by the incentives users have to participate and contribute measurements. With the aim of augmenting drive testing and significantly reducing its cost, we propose GenDT, a novel deep generative model that synthesizes high-fidelity time series of key radio network key performance indicators (KPIs). The training of GenDT relies on a relatively small amount of real-world measurement data along with corresponding and easily accessible network and environment context data. Through this, GenDT learns the relationship between context and radio network KPIs as they vary over time, and therefore trained GenDT model can subsequently be relied on to generate time series for different KPIs for new drive test routes (trajectories) without having to collect field measurements. GenDT represents an initial attempt at enabling efficient drive testing via generative modeling. Evaluations with real-world mobile network drive testing measurement datasets from two countries demonstrate that GenDT can synthesize significantly more dependable data than a range of baselines. We further show that GenDT has the potential to significantly reduce the drive testing related measurement effort, and that GenDT-generated data yields similar results to that with real data in the context of two downstream use cases - QoE prediction and handover analysis.},
booktitle = {Proceedings of the 18th International Conference on Emerging Networking EXperiments and Technologies},
pages = {43–58},
numpages = {16},
keywords = {conditional GANs, deep generative modeling, mobile network drive test measurement data, synthetic data generation},
location = {<conf-loc>, <city>Roma</city>, <country>Italy</country>, </conf-loc>},
series = {CoNEXT '22}
}

@inproceedings{10.1145/3324884.3416668,
author = {Nguyen, Hoang Lam and Nassar, Nebras and Kehrer, Timo and Grunske, Lars},
title = {MoFuzz: a fuzzer suite for testing model-driven software engineering tools},
year = {2021},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3416668},
doi = {10.1145/3324884.3416668},
abstract = {Fuzzing or fuzz testing is an established technique that aims to discover unexpected program behavior (e.g., bugs, security vulnerabilities, or crashes) by feeding automatically generated data into a program under test. However, the application of fuzzing to test Model-Driven Software Engineering (MDSE) tools is still limited because of the difficulty of existing fuzzers to provide structured, well-typed inputs, namely models that conform to typing and consistency constraints induced by a given meta-model and underlying modeling framework. By drawing from recent advances on both fuzz testing and automated model generation, we present three different approaches for fuzzing MDSE tools: A graph grammar-based fuzzer and two variants of a coverage-guided mutation-based fuzzer working with different sets of model mutation operators. Our evaluation on a set of real-world MDSE tools shows that our approaches can outperform both standard fuzzers and model generators w.r.t. their fuzzing capabilities. Moreover, we found that each of our approaches comes with its own strengths and weaknesses in terms of fault finding capabilities and the ability to cover different aspects of the system under test. Thus the approaches complement each other, forming a fuzzer suite for testing MDSE tools.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1103–1115},
numpages = {13},
keywords = {automated model generation, eclipse modeling framework, fuzzing, model-driven software engineering, modeling tools},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@inproceedings{10.1145/3524844.3528081,
author = {Quin, Federico and Weyns, Danny},
title = {SEAByTE: a self-adaptive micro-service system artifact for automating A/B testing},
year = {2022},
isbn = {9781450393058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524844.3528081},
doi = {10.1145/3524844.3528081},
abstract = {Micro-services are a common architectural approach to software development today. An indispensable tool for evolving micro-service systems is A/B testing. In A/B testing, two variants, A and B, are applied in an experimental setting. By measuring the outcome of an evaluation criterion, developers can make evidence-based decisions to guide the evolution of their software. Recent studies highlight the need for enhancing the automation when such experiments are conducted in iterations. To that end, we contribute a novel artifact that aims at enhancing the automation of an experimentation pipeline of a micro-service system relying on the principles of self-adaptation. Concretely, we propose SEAByTE, an experimental framework for testing novel self-adaptation solutions to enhance the automation of continuous A/B testing of a micro-service based system. We illustrate the use of the SEAByTE artifact with a concrete example.},
booktitle = {Proceedings of the 17th Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {77–83},
numpages = {7},
location = {Pittsburgh, Pennsylvania},
series = {SEAMS '22}
}

@inproceedings{10.1145/3477314.3507082,
author = {Misono, Masanori and Hatanaka, Toshiki and Shinagawa, Takahiro},
title = {DMAFV: testing device drivers against DMA faults},
year = {2022},
isbn = {9781450387132},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477314.3507082},
doi = {10.1145/3477314.3507082},
abstract = {A device may produce invalid values due to a malfunction or infection with malicious firmware. On the other hand, many device drivers implicitly assume that devices will conform to specifications and often do not sufficiently check device input values. One approach to detect faulty device drivers is using fault injection. However, previous studies targeted the MMIO (memory mapped I/O) area and the I/O instructions, but not the DMA (direct memory access) area.In this study, we propose a novel way to test device drivers against DMA faults by performing fault injection of the DMAed data. The proposed method identifies the DMA region by consulting the device's registers. Fault injection is realized by trapping memory accesses to the DMA region from the OS using a hypervisor and returning a fault-injected value. To reduce the overhead of the hypervisor, we use a thin hypervisor for fault injection.Using the proposed method, we found one bug in the Linux NVMe driver. The bug had been reported and confirmed by a developer and fixed later. We evaluate the overhead of the method and it is small enough to allow for its practical use.},
booktitle = {Proceedings of the 37th ACM/SIGAPP Symposium on Applied Computing},
pages = {1176–1182},
numpages = {7},
keywords = {DMA, device driver, fault injection, security},
location = {Virtual Event},
series = {SAC '22}
}

@inproceedings{10.1109/ICSE-SEIP52600.2021.00041,
author = {Zhang, Mingrui and Liu, Jianzhong and Ma, Fuchen and Zhang, Huafeng and Jiang, Yu},
title = {IntelliGen: automatic driver synthesis for fuzz testing},
year = {2021},
isbn = {9780738146690},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP52600.2021.00041},
doi = {10.1109/ICSE-SEIP52600.2021.00041},
abstract = {Fuzzing is a technique widely used in vulnerability detection. The process usually involves writing effective fuzz driver programs, which, when done manually, can be extremely labor intensive. Previous attempts at automation leave much to be desired, in either degree of automation or quality of output.In this paper, we propose IntelliGen, a framework that constructs valid fuzz drivers automatically. First, IntelliGen determines a set of entry functions and evaluates their respective chance of exhibiting a vulnerability. Then, IntelliGen generates fuzz drivers for the entry functions through hierarchical parameter replacement and type inference. We implemented IntelliGen and evaluated its effectiveness on real-world programs selected from the Android Open-Source Project, Google's fuzzer-test-suite and industrial collaborators. IntelliGen covered on average 1.08\texttimes{}-2.03\texttimes{} more basic blocks and 1.36\texttimes{}-2.06\texttimes{} more paths over state-of-the-art fuzz driver synthesizers FUDGE and FuzzGen.IntelliGen performed on par with manually written drivers and found 10 more bugs.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Software Engineering in Practice},
pages = {318–327},
numpages = {10},
keywords = {fuzz driver synthesis, fuzz testing, software analysis, vulnerability detection},
location = {Virtual Event, Spain},
series = {ICSE-SEIP '21}
}

@inproceedings{10.1145/3372799.3394363,
author = {He, Xiaoyu and Bauman, Erick and Li, Feng and Yu, Lei and Li, Linyu and Liu, Bingchang and Piao, Aihua and Hamlen, Kevin W. and Huo, Wei and Zou, Wei},
title = {Exploiting the Trust Between Boundaries: Discovering Memory Corruptions in Printers via Driver-Assisted Testing},
year = {2020},
isbn = {9781450370943},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372799.3394363},
doi = {10.1145/3372799.3394363},
abstract = {TrustScope is a new, a practical approach to identifying vulnerabilities in printer firmware without actually touching the firmware. By exploiting the trust between the firmware and the device drivers, TrustScope analyzes driver software to identify the driver endpoints that output the page description language (PDL) code to be sent to the printer, extracts key constraints for this output, generates new inputs violating these constraints, and fuzzes the printer firmware with malicious PDL code composed with these inputs yet conforming to the grammar of the PDL accepted by the printer. To accommodate the black-box nature of printers, printer behavior is observed strictly externally, allowing TrustScope to detect more vulnerabilities than only those that produce crashes. A variety of key optimizations, such as fuzzing without consuming paper and ink, and offline test case generation, make printer vulnerability detection feasible and practical. An implementation of TrustScope tested with 8 different printers reveals at least one test case causing anomalous behavior in every printer tested. For most printers it finds multiple vulnerabilities, 6 of which have been assigned CVE numbers, including buffer overflow and information disclosure.},
booktitle = {The 21st ACM SIGPLAN/SIGBED Conference on Languages, Compilers, and Tools for Embedded Systems},
pages = {74–84},
numpages = {11},
keywords = {printer, security, vulnerability},
location = {London, United Kingdom},
series = {LCTES '20}
}

